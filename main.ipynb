{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6Uk0tUfzwVxlv0dYxggIN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "517504d8039749d2a81727e9150605b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d8fcd6808644ee1bc7725a1d3a7e333",
              "IPY_MODEL_3598bd341f6a41acb163d2b0bc13ad16",
              "IPY_MODEL_7c7f5e9e7110420b81e6cd1bcc66a59b"
            ],
            "layout": "IPY_MODEL_7a3481e7c4d94e40854fc0cb1ab262bf"
          }
        },
        "9d8fcd6808644ee1bc7725a1d3a7e333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbf1c1794fff49fda7ed56d822e84fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_ae913af498594650867ebd5843d5834f",
            "value": "config.json: 100%"
          }
        },
        "3598bd341f6a41acb163d2b0bc13ad16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2072a86d141c4ad887fffca7fcd1eec1",
            "max": 892,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a47f6a9e3cf4480992ed0509183335dd",
            "value": 892
          }
        },
        "7c7f5e9e7110420b81e6cd1bcc66a59b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2bbcddfc82a4ad085397953d726e1aa",
            "placeholder": "​",
            "style": "IPY_MODEL_fcc74829380546379f89a5aa51036640",
            "value": " 892/892 [00:00&lt;00:00, 55.7kB/s]"
          }
        },
        "7a3481e7c4d94e40854fc0cb1ab262bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbf1c1794fff49fda7ed56d822e84fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae913af498594650867ebd5843d5834f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2072a86d141c4ad887fffca7fcd1eec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47f6a9e3cf4480992ed0509183335dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2bbcddfc82a4ad085397953d726e1aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc74829380546379f89a5aa51036640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "476bd5420a0f4e3590af40093648b632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66ef0bc7add141819c519e9c90b88fc2",
              "IPY_MODEL_b70ceb839b41424eb30e8b396682ea72",
              "IPY_MODEL_f116664e770743af8da3399ae8524c93"
            ],
            "layout": "IPY_MODEL_0305f9d328cd4b65b96dd899e277f945"
          }
        },
        "66ef0bc7add141819c519e9c90b88fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c6fc8a5fb0441bb472a00be949ab24",
            "placeholder": "​",
            "style": "IPY_MODEL_78ce6cad954b49519d0009c0d77b854f",
            "value": "model.safetensors: 100%"
          }
        },
        "b70ceb839b41424eb30e8b396682ea72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9bceb67dabb4b7297d2abb31402b4f8",
            "max": 2235440664,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2758542e21794dd38b29e426c4964f31",
            "value": 2235440664
          }
        },
        "f116664e770743af8da3399ae8524c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04ea1681f35c4968b17273426ca78cfa",
            "placeholder": "​",
            "style": "IPY_MODEL_b9b5846b1f67422497bada4cd8823940",
            "value": " 2.24G/2.24G [00:32&lt;00:00, 60.1MB/s]"
          }
        },
        "0305f9d328cd4b65b96dd899e277f945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c6fc8a5fb0441bb472a00be949ab24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ce6cad954b49519d0009c0d77b854f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9bceb67dabb4b7297d2abb31402b4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2758542e21794dd38b29e426c4964f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04ea1681f35c4968b17273426ca78cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b5846b1f67422497bada4cd8823940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5a6026e9aa44736ba6207c43dedb1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_317075e79cb148aca4adad556bb751d3",
              "IPY_MODEL_8e3a0359555247d7b0b339bd1b00a665",
              "IPY_MODEL_eed49d30f7ee4bdd876324dca8db4b0d"
            ],
            "layout": "IPY_MODEL_8f71a549394a422283e905ad79ec86e0"
          }
        },
        "317075e79cb148aca4adad556bb751d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57a8b0d03e724cd09333c446df5262fd",
            "placeholder": "​",
            "style": "IPY_MODEL_0622faf29f984d6eb6beb020b68de533",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8e3a0359555247d7b0b339bd1b00a665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e16084076aa240ad82cf72d4291ab2bf",
            "max": 406,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99b4a7cdcf3d401c912e7f6b3f78de63",
            "value": 406
          }
        },
        "eed49d30f7ee4bdd876324dca8db4b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d20d410313744be7a9fca0707b3a90ad",
            "placeholder": "​",
            "style": "IPY_MODEL_3dace62d8da9451ea370317f8d4464a5",
            "value": " 406/406 [00:00&lt;00:00, 43.0kB/s]"
          }
        },
        "8f71a549394a422283e905ad79ec86e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a8b0d03e724cd09333c446df5262fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0622faf29f984d6eb6beb020b68de533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e16084076aa240ad82cf72d4291ab2bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b4a7cdcf3d401c912e7f6b3f78de63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d20d410313744be7a9fca0707b3a90ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dace62d8da9451ea370317f8d4464a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93daf05fb7ee49d1ad9e276fa30856d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a0185284afb44b485c721d92f86f207",
              "IPY_MODEL_ee7b3c6de1f64aaa9d68d2771f4b87c7",
              "IPY_MODEL_8c7d7bfd00c64f7bb076b90d7f621174"
            ],
            "layout": "IPY_MODEL_e0746e9c7d1f4c24abc3969aaae3ff33"
          }
        },
        "3a0185284afb44b485c721d92f86f207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65e62518619d4a45bb596613a74ec178",
            "placeholder": "​",
            "style": "IPY_MODEL_3e906831985d45cda4e856676220fcfd",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "ee7b3c6de1f64aaa9d68d2771f4b87c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f90dcd4da5804c3cbca57298b404c132",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_baea2f6f58bd497f9c154d65e7f5b120",
            "value": 5069051
          }
        },
        "8c7d7bfd00c64f7bb076b90d7f621174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_822545be41714e5291d046ce220b2411",
            "placeholder": "​",
            "style": "IPY_MODEL_7e55e323a8aa425da005b3208f680e0a",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 9.60MB/s]"
          }
        },
        "e0746e9c7d1f4c24abc3969aaae3ff33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e62518619d4a45bb596613a74ec178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e906831985d45cda4e856676220fcfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f90dcd4da5804c3cbca57298b404c132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baea2f6f58bd497f9c154d65e7f5b120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "822545be41714e5291d046ce220b2411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e55e323a8aa425da005b3208f680e0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2a4ea28ba824340ab56cb178597e564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5eaf52c440fc49c8ac6f307b91abad6d",
              "IPY_MODEL_fe282e01871948e0bc34fca923fa94e6",
              "IPY_MODEL_90d2a1be326f44fe9dc2004446f8abf6"
            ],
            "layout": "IPY_MODEL_22b8c35dd9354beea8cb661bfd43634e"
          }
        },
        "5eaf52c440fc49c8ac6f307b91abad6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66f93f8984a74790b015443d222e3588",
            "placeholder": "​",
            "style": "IPY_MODEL_5050112d34094c7c91c22f8cad906386",
            "value": "tokenizer.json: 100%"
          }
        },
        "fe282e01871948e0bc34fca923fa94e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d547d1eb2dd4eebba75bb62c541eb27",
            "max": 17098080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1c27199d4b240fea13d8cd49f18a702",
            "value": 17098080
          }
        },
        "90d2a1be326f44fe9dc2004446f8abf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e1c10f144204ffeba3a54ec55959beb",
            "placeholder": "​",
            "style": "IPY_MODEL_3e895b19a36b472799287c26cf790841",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 26.0MB/s]"
          }
        },
        "22b8c35dd9354beea8cb661bfd43634e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66f93f8984a74790b015443d222e3588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5050112d34094c7c91c22f8cad906386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d547d1eb2dd4eebba75bb62c541eb27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1c27199d4b240fea13d8cd49f18a702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e1c10f144204ffeba3a54ec55959beb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e895b19a36b472799287c26cf790841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25e626a201f4451583535339b25c1c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d4a0cd2a5e94d8f8c1690c49b887a85",
              "IPY_MODEL_6faec6565fb84d429deea644fa3bbdc1",
              "IPY_MODEL_95663d9ee6e1442a819ec23de0926f5a"
            ],
            "layout": "IPY_MODEL_1e0f25ff4cf640a494f6d0030b13e413"
          }
        },
        "4d4a0cd2a5e94d8f8c1690c49b887a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_798a09f3d09842c2b3ba308e195e666f",
            "placeholder": "​",
            "style": "IPY_MODEL_88c9865737d34ed8bd22f37c3aeebdc5",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "6faec6565fb84d429deea644fa3bbdc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac0e91732a314eb281ef1adf5869a97c",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7269606d40143f7a2775ae9a30c5bc9",
            "value": 239
          }
        },
        "95663d9ee6e1442a819ec23de0926f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb133df0264d481b89792a11152fc2ee",
            "placeholder": "​",
            "style": "IPY_MODEL_9bddc141e3894839b7610321be74ac18",
            "value": " 239/239 [00:00&lt;00:00, 17.7kB/s]"
          }
        },
        "1e0f25ff4cf640a494f6d0030b13e413": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "798a09f3d09842c2b3ba308e195e666f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c9865737d34ed8bd22f37c3aeebdc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac0e91732a314eb281ef1adf5869a97c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7269606d40143f7a2775ae9a30c5bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb133df0264d481b89792a11152fc2ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bddc141e3894839b7610321be74ac18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vxhnu/YouTube-Video-Summarizer/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vBOTOWmgAIE",
        "outputId": "2713168f-50c3-4a6b-81d4-8dbdd8d1ae04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "sT5DQ-JUgGK-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_id(url_link):\n",
        "    return url_link.split(\"watch?v=\")[-1]"
      ],
      "metadata": {
        "id": "UKba-01fgGIG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = get_video_id(\"7owOKKgT0ak\")"
      ],
      "metadata": {
        "id": "KTLJ9H9mgGGZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = YouTubeTranscriptApi.get_transcript(video_id)"
      ],
      "metadata": {
        "id": "6NBk_d3GgGCs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIBz-q20gF_i",
        "outputId": "bc86c9d1-978a-4e4e-84a4-20dc658f1729"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': \"hello everyone I'm Santi and I'm\", 'start': 0.28, 'duration': 4.24},\n",
              " {'text': 'currently working as an ml engineer so',\n",
              "  'start': 2.399,\n",
              "  'duration': 3.601},\n",
              " {'text': \"I'll be discussing the derivation and\",\n",
              "  'start': 4.52,\n",
              "  'duration': 3.6},\n",
              " {'text': 'math behind classification which is',\n",
              "  'start': 6.0,\n",
              "  'duration': 5.0},\n",
              " {'text': 'another top ml algorithm I have covered',\n",
              "  'start': 8.12,\n",
              "  'duration': 5.88},\n",
              " {'text': 'log likelihood before in this video and',\n",
              "  'start': 11.0,\n",
              "  'duration': 5.24},\n",
              " {'text': \"I would say it's kind of a prerequisite\",\n",
              "  'start': 14.0,\n",
              "  'duration': 3.88},\n",
              " {'text': 'for this video if you already know about',\n",
              "  'start': 16.24,\n",
              "  'duration': 4.32},\n",
              " {'text': \"that then without any further Ado let's\",\n",
              "  'start': 17.88,\n",
              "  'duration': 5.719},\n",
              " {'text': 'start so the first thing that comes is',\n",
              "  'start': 20.56,\n",
              "  'duration': 4.559},\n",
              " {'text': 'what is the problem that we are trying',\n",
              "  'start': 23.599,\n",
              "  'duration': 4.52},\n",
              " {'text': 'to tackle here so basically we have so',\n",
              "  'start': 25.119,\n",
              "  'duration': 4.681},\n",
              " {'text': 'basically the first question always is',\n",
              "  'start': 28.119,\n",
              "  'duration': 4.12},\n",
              " {'text': 'what the data set what does the data set',\n",
              "  'start': 29.8,\n",
              "  'duration': 3.16},\n",
              " {'text': 'look', 'start': 32.239, 'duration': 5.0},\n",
              " {'text': 'like so it looks something like',\n",
              "  'start': 32.96,\n",
              "  'duration': 4.279},\n",
              " {'text': \"this as I said we'll start with the\",\n",
              "  'start': 38.16,\n",
              "  'duration': 7.2},\n",
              " {'text': 'simplest form first so X1 X2 and Y so Y',\n",
              "  'start': 40.68,\n",
              "  'duration': 7.08},\n",
              " {'text': \"is basically uh either zero or one we'll\",\n",
              "  'start': 45.36,\n",
              "  'duration': 5.359},\n",
              " {'text': 'be tackling binary classification first',\n",
              "  'start': 47.76,\n",
              "  'duration': 6.04},\n",
              " {'text': \"so why is either zero or one let's say\",\n",
              "  'start': 50.719,\n",
              "  'duration': 5.64},\n",
              " {'text': 'looks something like this and X1 could',\n",
              "  'start': 53.8,\n",
              "  'duration': 7.32},\n",
              " {'text': 'be 1 2 3 4 maybe and X2 could be',\n",
              "  'start': 56.359,\n",
              "  'duration': 7.361},\n",
              " {'text': 'anything um.', 'start': 61.12, 'duration': 7.28},\n",
              " {'text': '5.9 1.0 1.1 anything so these X1 X2 are',\n",
              "  'start': 63.72,\n",
              "  'duration': 6.96},\n",
              " {'text': 'the two features and Y is our Target',\n",
              "  'start': 68.4,\n",
              "  'duration': 3.96},\n",
              " {'text': 'variable which is either zero or one and',\n",
              "  'start': 70.68,\n",
              "  'duration': 5.88},\n",
              " {'text': 'we want to predict y given X1 and X2 so',\n",
              "  'start': 72.36,\n",
              "  'duration': 6.119},\n",
              " {'text': 'now comes the graph exactly what are we',\n",
              "  'start': 76.56,\n",
              "  'duration': 5.72},\n",
              " {'text': \"trying to find here so we'll try to plot\",\n",
              "  'start': 78.479,\n",
              "  'duration': 8.161},\n",
              " {'text': \"X1 X2 here X1 and X2 and then we'll try\",\n",
              "  'start': 82.28,\n",
              "  'duration': 6.44},\n",
              " {'text': 'to get', 'start': 86.64, 'duration': 5.24},\n",
              " {'text': \"the uh points here so let's just\",\n",
              "  'start': 88.72,\n",
              "  'duration': 6.679},\n",
              " {'text': 'say these These are the four points that',\n",
              "  'start': 91.88,\n",
              "  'duration': 5.599},\n",
              " {'text': \"we have so now what we're trying to find\",\n",
              "  'start': 95.399,\n",
              "  'duration': 5.961},\n",
              " {'text': 'is this line because this line would be',\n",
              "  'start': 97.479,\n",
              "  'duration': 6.28},\n",
              " {'text': 'distinguishing these points suppose this',\n",
              "  'start': 101.36,\n",
              "  'duration': 6.6},\n",
              " {'text': 'point has y equal to um one and this',\n",
              "  'start': 103.759,\n",
              "  'duration': 6.281},\n",
              " {'text': 'point has y equal 1 this point has y',\n",
              "  'start': 107.96,\n",
              "  'duration': 4.6},\n",
              " {'text': 'equal 0 this point has y equal 0 so this',\n",
              "  'start': 110.04,\n",
              "  'duration': 4.039},\n",
              " {'text': \"is the line that we're trying to find\",\n",
              "  'start': 112.56,\n",
              "  'duration': 5.28},\n",
              " {'text': 'that uh differentiates the one yal 1',\n",
              "  'start': 114.079,\n",
              "  'duration': 6.161},\n",
              " {'text': 'from yal 0 because suppose now we have',\n",
              "  'start': 117.84,\n",
              "  'duration': 4.72},\n",
              " {'text': 'any kind of Point suppose X1 is here',\n",
              "  'start': 120.24,\n",
              "  'duration': 4.519},\n",
              " {'text': 'something and we have given X1 and X2',\n",
              "  'start': 122.56,\n",
              "  'duration': 4.28},\n",
              " {'text': 'and we find it here and we know that',\n",
              "  'start': 124.759,\n",
              "  'duration': 4.0},\n",
              " {'text': 'this point lies on this side of the line',\n",
              "  'start': 126.84,\n",
              "  'duration': 5.399},\n",
              " {'text': \"so y will be one it's as simple as that\",\n",
              "  'start': 128.759,\n",
              "  'duration': 5.801},\n",
              " {'text': 'I hope you get this so basically X1 and',\n",
              "  'start': 132.239,\n",
              "  'duration': 5.0},\n",
              " {'text': 'X2 given X1 and X2 what is our y that is',\n",
              "  'start': 134.56,\n",
              "  'duration': 5.84},\n",
              " {'text': 'our Target to find given X1 and X2 we',\n",
              "  'start': 137.239,\n",
              "  'duration': 7.841},\n",
              " {'text': 'want to find y fun fact um so logistic',\n",
              "  'start': 140.4,\n",
              "  'duration': 6.919},\n",
              " {'text': 'degression is also called classification',\n",
              "  'start': 145.08,\n",
              "  'duration': 4.439},\n",
              " {'text': 'do you know why um drop your thoughts in',\n",
              "  'start': 147.319,\n",
              "  'duration': 3.801},\n",
              " {'text': 'the comments', 'start': 149.519, 'duration': 3.641},\n",
              " {'text': 'okay coming back to this one so this',\n",
              "  'start': 151.12,\n",
              "  'duration': 6.56},\n",
              " {'text': 'line in um classification is', 'start': 153.16, 'duration': 6.52},\n",
              " {'text': 'called', 'start': 157.68, 'duration': 4.199},\n",
              " {'text': 'decision', 'start': 159.68, 'duration': 4.8},\n",
              " {'text': 'bound which is very important remember',\n",
              "  'start': 161.879,\n",
              "  'duration': 4.761},\n",
              " {'text': 'this term so what is the definition of',\n",
              "  'start': 164.48,\n",
              "  'duration': 4.0},\n",
              " {'text': 'this line this line is the maximum',\n",
              "  'start': 166.64,\n",
              "  'duration': 3.92},\n",
              " {'text': 'distance from all of the points so what',\n",
              "  'start': 168.48,\n",
              "  'duration': 3.759},\n",
              " {'text': 'do I mean by that so here actually we',\n",
              "  'start': 170.56,\n",
              "  'duration': 3.72},\n",
              " {'text': 'are tracking the distance between the',\n",
              "  'start': 172.239,\n",
              "  'duration': 7.36},\n",
              " {'text': 'points so so these', 'start': 174.28, 'duration': 5.319},\n",
              " {'text': 'these', 'start': 180.8, 'duration': 3.719},\n",
              " {'text': 'differences these are this is the DI for',\n",
              "  'start': 181.76,\n",
              "  'duration': 5.6},\n",
              " {'text': 'each point X1 I and X2 I we have this',\n",
              "  'start': 184.519,\n",
              "  'duration': 6.401},\n",
              " {'text': 'point which is Di and this di we want to',\n",
              "  'start': 187.36,\n",
              "  'duration': 6.439},\n",
              " {'text': 'maximize because we want the line such',\n",
              "  'start': 190.92,\n",
              "  'duration': 5.679},\n",
              " {'text': 'that it is it it differentiates the two',\n",
              "  'start': 193.799,\n",
              "  'duration': 7.561},\n",
              " {'text': 'uh one and zero completely', 'start': 196.599, 'duration': 7.241},\n",
              " {'text': 'okay so what is the exactly the equation',\n",
              "  'start': 201.36,\n",
              "  'duration': 6.159},\n",
              " {'text': \"that we're trying to solve here so y i\",\n",
              "  'start': 203.84,\n",
              "  'duration': 7.8},\n",
              " {'text': 'given x i param', 'start': 207.519, 'duration': 5.64},\n",
              " {'text': 'this is the equation that we trying to',\n",
              "  'start': 211.64,\n",
              "  'duration': 3.679},\n",
              " {'text': 'find why are we trying to find that',\n",
              "  'start': 213.159,\n",
              "  'duration': 5.921},\n",
              " {'text': 'because if you remember correctly this',\n",
              "  'start': 215.319,\n",
              "  'duration': 6.64},\n",
              " {'text': 'is log of', 'start': 219.08, 'duration': 7.96},\n",
              " {'text': 'this um summation I = 1 to m m being the',\n",
              "  'start': 221.959,\n",
              "  'duration': 8.121},\n",
              " {'text': 'total number of data', 'start': 227.04, 'duration': 3.04},\n",
              " {'text': 'points M being the total number of data',\n",
              "  'start': 230.159,\n",
              "  'duration': 4.36},\n",
              " {'text': 'points so this equation is the log',\n",
              "  'start': 235.799,\n",
              "  'duration': 4.601},\n",
              " {'text': 'likelihood so this is is very important',\n",
              "  'start': 238.2,\n",
              "  'duration': 3.8},\n",
              " {'text': 'so that is why we trying to find this',\n",
              "  'start': 240.4,\n",
              "  'duration': 3.559},\n",
              " {'text': 'and we trying to Define this', 'start': 242.0, 'duration': 6.519},\n",
              " {'text': 'term so this is actually y i given XI',\n",
              "  'start': 243.959,\n",
              "  'duration': 6.681},\n",
              " {'text': 'parameterized by Theta this is actually',\n",
              "  'start': 248.519,\n",
              "  'duration': 5.0},\n",
              " {'text': 'the Bol', 'start': 250.64, 'duration': 2.879},\n",
              " {'text': 'distribution because either the',\n",
              "  'start': 256.519,\n",
              "  'duration': 3.041},\n",
              " {'text': 'probability we are trying to kind of',\n",
              "  'start': 258.199,\n",
              "  'duration': 3.6},\n",
              " {'text': 'find the probability of it being one or',\n",
              "  'start': 259.56,\n",
              "  'duration': 4.76},\n",
              " {'text': 'zero and b means only two values we can',\n",
              "  'start': 261.799,\n",
              "  'duration': 4.601},\n",
              " {'text': 'predict one or zero so that is why this',\n",
              "  'start': 264.32,\n",
              "  'duration': 6.159},\n",
              " {'text': 'is like similar to the Bol distribution',\n",
              "  'start': 266.4,\n",
              "  'duration': 5.6},\n",
              " {'text': 'now that we have got this out of the way',\n",
              "  'start': 270.479,\n",
              "  'duration': 3.521},\n",
              " {'text': \"we know what we're trying to\", 'start': 272.0, 'duration': 5.72},\n",
              " {'text': \"find so now comes the DI let's focus on\",\n",
              "  'start': 274.0,\n",
              "  'duration': 6.199},\n",
              " {'text': \"di let's get an equation for finding out\",\n",
              "  'start': 277.72,\n",
              "  'duration': 4.68},\n",
              " {'text': 'Di and formalizing', 'start': 280.199, 'duration': 6.481},\n",
              " {'text': 'that so if you remember I just now',\n",
              "  'start': 282.4,\n",
              "  'duration': 4.28},\n",
              " {'text': 'said these are all di so this line this',\n",
              "  'start': 287.919,\n",
              "  'duration': 6.881},\n",
              " {'text': 'side is a positive side of the line and',\n",
              "  'start': 292.4,\n",
              "  'duration': 3.799},\n",
              " {'text': 'this side is the negative side of the',\n",
              "  'start': 294.8,\n",
              "  'duration': 6.839},\n",
              " {'text': 'line so we can either decide um',\n",
              "  'start': 296.199,\n",
              "  'duration': 5.44},\n",
              " {'text': 'so we can either', 'start': 302.12, 'duration': 4.359},\n",
              " {'text': 'decide we can decide the direction so',\n",
              "  'start': 304.039,\n",
              "  'duration': 4.041},\n",
              " {'text': \"let's just say this side of the line is\",\n",
              "  'start': 306.479,\n",
              "  'duration': 3.44},\n",
              " {'text': 'positive and this side of the line is',\n",
              "  'start': 308.08,\n",
              "  'duration': 5.04},\n",
              " {'text': 'negative and then this', 'start': 309.919, 'duration': 9.12},\n",
              " {'text': \"di let's say let's denote Z as di so now\",\n",
              "  'start': 313.12,\n",
              "  'duration': 7.56},\n",
              " {'text': 'this is very important to understand so',\n",
              "  'start': 319.039,\n",
              "  'duration': 6.041},\n",
              " {'text': 'listen carefully so this di it can range',\n",
              "  'start': 320.68,\n",
              "  'duration': 7.12},\n",
              " {'text': 'from positive Infinity to minus infinity',\n",
              "  'start': 325.08,\n",
              "  'duration': 4.959},\n",
              " {'text': 'so we trying to find the function so can',\n",
              "  'start': 327.8,\n",
              "  'duration': 4.48},\n",
              " {'text': 'range from positive Infinity to negative',\n",
              "  'start': 330.039,\n",
              "  'duration': 6.16},\n",
              " {'text': \"Infinity okay so now if di is zero let's\",\n",
              "  'start': 332.28,\n",
              "  'duration': 5.6},\n",
              " {'text': 'say for a point the point is lying on',\n",
              "  'start': 336.199,\n",
              "  'duration': 3.081},\n",
              " {'text': \"the line so we don't know whether it's\",\n",
              "  'start': 337.88,\n",
              "  'duration': 3.12},\n",
              " {'text': \"positive or negative whether it's\",\n",
              "  'start': 339.28,\n",
              "  'duration': 4.08},\n",
              " {'text': \"whether it's one or zero let's just say\",\n",
              "  'start': 341.0,\n",
              "  'duration': 4.56},\n",
              " {'text': 'positive one gets classified as one',\n",
              "  'start': 343.36,\n",
              "  'duration': 4.559},\n",
              " {'text': 'negative is classified as zero so if the',\n",
              "  'start': 345.56,\n",
              "  'duration': 3.96},\n",
              " {'text': \"point is on the line we don't know\",\n",
              "  'start': 347.919,\n",
              "  'duration': 3.0},\n",
              " {'text': \"whether it's positive or negative we\",\n",
              "  'start': 349.52,\n",
              "  'duration': 2.799},\n",
              " {'text': \"don't know whether it's one or zero so\",\n",
              "  'start': 350.919,\n",
              "  'duration': 4.081},\n",
              " {'text': 'the probability is half so di equal 0',\n",
              "  'start': 352.319,\n",
              "  'duration': 5.32},\n",
              " {'text': 'should have the probability as half we',\n",
              "  'start': 355.0,\n",
              "  'duration': 4.4},\n",
              " {'text': 'are now trying to find the constraints',\n",
              "  'start': 357.639,\n",
              "  'duration': 3.4},\n",
              " {'text': 'for for finding the function that will',\n",
              "  'start': 359.4,\n",
              "  'duration': 5.519},\n",
              " {'text': 'fit di perfectly so di0 p is equal to',\n",
              "  'start': 361.039,\n",
              "  'duration': 7.041},\n",
              " {'text': 'half what if di is positive Infinity',\n",
              "  'start': 364.919,\n",
              "  'duration': 6.921},\n",
              " {'text': 'which means that probability will be',\n",
              "  'start': 368.08,\n",
              "  'duration': 6.76},\n",
              " {'text': \"one and then di suppose it's negative\",\n",
              "  'start': 371.84,\n",
              "  'duration': 5.28},\n",
              " {'text': 'Infinity then probability will be zero',\n",
              "  'start': 374.84,\n",
              "  'duration': 5.44},\n",
              " {'text': 'of getting classified as one we are',\n",
              "  'start': 377.12,\n",
              "  'duration': 5.04},\n",
              " {'text': 'finding that we are now currently',\n",
              "  'start': 380.28,\n",
              "  'duration': 4.28},\n",
              " {'text': 'thinking of p as the probability of',\n",
              "  'start': 382.16,\n",
              "  'duration': 6.08},\n",
              " {'text': 'getting classified as one this one so if',\n",
              "  'start': 384.56,\n",
              "  'duration': 5.56},\n",
              " {'text': \"it's on the line then it see and\",\n",
              "  'start': 388.24,\n",
              "  'duration': 4.079},\n",
              " {'text': \"probability is half if it's Infinity\",\n",
              "  'start': 390.12,\n",
              "  'duration': 4.6},\n",
              " {'text': \"which means we know for sure that it's\",\n",
              "  'start': 392.319,\n",
              "  'duration': 5.32},\n",
              " {'text': 'one point is one so p is equal to 1 and',\n",
              "  'start': 394.72,\n",
              "  'duration': 4.4},\n",
              " {'text': \"if it's negative Infinity we know for\",\n",
              "  'start': 397.639,\n",
              "  'duration': 4.041},\n",
              " {'text': 'sure that the point can never be one so',\n",
              "  'start': 399.12,\n",
              "  'duration': 6.0},\n",
              " {'text': \"it's B is zero so now what function fits\",\n",
              "  'start': 401.68,\n",
              "  'duration': 6.359},\n",
              " {'text': 'all these conditions yes so that is the',\n",
              "  'start': 405.12,\n",
              "  'duration': 5.56},\n",
              " {'text': 'sigmoid', 'start': 408.039, 'duration': 2.641},\n",
              " {'text': 'function write this down this is the',\n",
              "  'start': 411.16,\n",
              "  'duration': 6.039},\n",
              " {'text': 'sigmoid function so what does this look',\n",
              "  'start': 414.68,\n",
              "  'duration': 5.519},\n",
              " {'text': 'like', 'start': 417.199, 'duration': 3.0},\n",
              " {'text': 'so now we going to see how the sigmo',\n",
              "  'start': 420.84,\n",
              "  'duration': 3.96},\n",
              " {'text': 'function looks like we denote It',\n",
              "  'start': 422.639,\n",
              "  'duration': 4.641},\n",
              " {'text': 'generally as G as a function of', 'start': 424.8, 'duration': 7.64},\n",
              " {'text': 'Z it would be 1 upon 1 + e^ minus Z this',\n",
              "  'start': 427.28,\n",
              "  'duration': 7.039},\n",
              " {'text': \"is a sigmoid function so let's put in\",\n",
              "  'start': 432.44,\n",
              "  'duration': 4.28},\n",
              " {'text': 'the values and see if it satisfies it or',\n",
              "  'start': 434.319,\n",
              "  'duration': 5.28},\n",
              " {'text': 'not that we just now saw so if Z equals',\n",
              "  'start': 436.72,\n",
              "  'duration': 7.599},\n",
              " {'text': 'z then what is gz it is half perfect if',\n",
              "  'start': 439.599,\n",
              "  'duration': 8.801},\n",
              " {'text': 'Z is infinity what is gz gz is one',\n",
              "  'start': 444.319,\n",
              "  'duration': 7.241},\n",
              " {'text': 'perfect because because 1 by 1 + e^',\n",
              "  'start': 448.4,\n",
              "  'duration': 4.919},\n",
              " {'text': 'minus infinity this would tend to zero',\n",
              "  'start': 451.56,\n",
              "  'duration': 5.16},\n",
              " {'text': 'so we have 1 and if Z is minus',\n",
              "  'start': 453.319,\n",
              "  'duration': 8.081},\n",
              " {'text': 'infinity then g z is equal to Z why',\n",
              "  'start': 456.72,\n",
              "  'duration': 8.879},\n",
              " {'text': 'because 1 by 1 + E power Infinity this',\n",
              "  'start': 461.4,\n",
              "  'duration': 6.079},\n",
              " {'text': 'is this tends to Infinity so 1 by',\n",
              "  'start': 465.599,\n",
              "  'duration': 4.241},\n",
              " {'text': 'Infinity is zero so now we have a',\n",
              "  'start': 467.479,\n",
              "  'duration': 4.081},\n",
              " {'text': 'function which satisfies all these',\n",
              "  'start': 469.84,\n",
              "  'duration': 4.199},\n",
              " {'text': 'points so now we have our idle function',\n",
              "  'start': 471.56,\n",
              "  'duration': 4.12},\n",
              " {'text': 'so Sigmar function is what we are going',\n",
              "  'start': 474.039,\n",
              "  'duration': 5.401},\n",
              " {'text': 'to use here', 'start': 475.68, 'duration': 3.76},\n",
              " {'text': \"before I delete this let's see the curve\",\n",
              "  'start': 479.56,\n",
              "  'duration': 4.639},\n",
              " {'text': 'of the sigmoid', 'start': 481.879, 'duration': 6.32},\n",
              " {'text': 'function so this is one this is Zer this',\n",
              "  'start': 484.199,\n",
              "  'duration': 10.641},\n",
              " {'text': 'is half so at half we have um at half we',\n",
              "  'start': 488.199,\n",
              "  'duration': 9.881},\n",
              " {'text': 'at zero Z equal to Z so this is z this',\n",
              "  'start': 494.84,\n",
              "  'duration': 4.039},\n",
              " {'text': 'is', 'start': 498.08, 'duration': 3.839},\n",
              " {'text': 'gz so when Z equal to Z then we have',\n",
              "  'start': 498.879,\n",
              "  'duration': 5.561},\n",
              " {'text': 'half when Z equal to Infinity we have',\n",
              "  'start': 501.919,\n",
              "  'duration': 5.441},\n",
              " {'text': 'one and when Z equal to minus infinity',\n",
              "  'start': 504.44,\n",
              "  'duration': 6.0},\n",
              " {'text': 'we have zero so somewhat like', 'start': 507.36, 'duration': 5.32},\n",
              " {'text': 'this is the sigmoid curve remember this',\n",
              "  'start': 510.44,\n",
              "  'duration': 4.12},\n",
              " {'text': 'is very important comes in interviews',\n",
              "  'start': 512.68,\n",
              "  'duration': 4.32},\n",
              " {'text': 'comes in placements wherever jobs',\n",
              "  'start': 514.56,\n",
              "  'duration': 5.32},\n",
              " {'text': 'internships this is extremely important',\n",
              "  'start': 517.0,\n",
              "  'duration': 6.039},\n",
              " {'text': 'College exams gate exams whatever so',\n",
              "  'start': 519.88,\n",
              "  'duration': 5.639},\n",
              " {'text': 'this is very important um so now we have',\n",
              "  'start': 523.039,\n",
              "  'duration': 4.961},\n",
              " {'text': 'the sigmoid function out of our way',\n",
              "  'start': 525.519,\n",
              "  'duration': 4.161},\n",
              " {'text': 'great one', 'start': 528.0, 'duration': 4.08},\n",
              " {'text': \"down so now we'll find out the log\",\n",
              "  'start': 529.68,\n",
              "  'duration': 7.92},\n",
              " {'text': 'likelihood so probability of Yi = to 1',\n",
              "  'start': 532.08,\n",
              "  'duration': 8.759},\n",
              " {'text': 'given x i parameter Iz by Theta', 'start': 537.6, 'duration': 5.16},\n",
              " {'text': 'probability of y equal 1 given x i',\n",
              "  'start': 540.839,\n",
              "  'duration': 3.401},\n",
              " {'text': 'parameterized by Theta earli it was',\n",
              "  'start': 542.76,\n",
              "  'duration': 3.84},\n",
              " {'text': 'probability of Yi given XI parameterized',\n",
              "  'start': 544.24,\n",
              "  'duration': 4.56},\n",
              " {'text': 'by Theta because Yi was continuous in',\n",
              "  'start': 546.6,\n",
              "  'duration': 3.56},\n",
              " {'text': \"distinct case it's a little bit\", 'start': 548.8, 'duration': 3.8},\n",
              " {'text': \"different so we'll take yal yal 1 and\",\n",
              "  'start': 550.16,\n",
              "  'duration': 4.16},\n",
              " {'text': \"we'll take y equal 0 and we'll get the\",\n",
              "  'start': 552.6,\n",
              "  'duration': 3.679},\n",
              " {'text': \"formula for these two then we'll see\",\n",
              "  'start': 554.32,\n",
              "  'duration': 3.68},\n",
              " {'text': 'what to do how to combine them together',\n",
              "  'start': 556.279,\n",
              "  'duration': 4.401},\n",
              " {'text': \"that's very interesting as well we'll\",\n",
              "  'start': 558.0,\n",
              "  'duration': 6.04},\n",
              " {'text': 'come to that so this is the yal 1 given',\n",
              "  'start': 560.68,\n",
              "  'duration': 5.76},\n",
              " {'text': 'XI parameterized by Theta so we know we',\n",
              "  'start': 564.04,\n",
              "  'duration': 5.919},\n",
              " {'text': 'just now found this 1 + e ^', 'start': 566.44, 'duration': 5.88},\n",
              " {'text': 'so now what is this term what is this',\n",
              "  'start': 569.959,\n",
              "  'duration': 4.721},\n",
              " {'text': 'term so basically if you remember when I',\n",
              "  'start': 572.32,\n",
              "  'duration': 4.079},\n",
              " {'text': 'was talking about the decision boundary',\n",
              "  'start': 574.68,\n",
              "  'duration': 3.399},\n",
              " {'text': 'in binary classification is going to be',\n",
              "  'start': 576.399,\n",
              "  'duration': 3.961},\n",
              " {'text': 'a straight line obviously because we',\n",
              "  'start': 578.079,\n",
              "  'duration': 4.361},\n",
              " {'text': 'trying to divide these', 'start': 580.36, 'duration': 5.28},\n",
              " {'text': 'points this distance we trying to',\n",
              "  'start': 582.44,\n",
              "  'duration': 5.2},\n",
              " {'text': \"maximize this distance we're trying to\",\n",
              "  'start': 585.64,\n",
              "  'duration': 4.24},\n",
              " {'text': 'maximize from all the points so this is',\n",
              "  'start': 587.64,\n",
              "  'duration': 3.8},\n",
              " {'text': 'a straight line so now we know for a',\n",
              "  'start': 589.88,\n",
              "  'duration': 2.72},\n",
              " {'text': 'function which is the equation of a',\n",
              "  'start': 591.44,\n",
              "  'duration': 5.48},\n",
              " {'text': 'straight line is y equal to mx + C but',\n",
              "  'start': 592.6,\n",
              "  'duration': 7.44},\n",
              " {'text': \"this is only when uh it's a it has only\",\n",
              "  'start': 596.92,\n",
              "  'duration': 6.56},\n",
              " {'text': 'one feature which is X so we have here',\n",
              "  'start': 600.04,\n",
              "  'duration': 5.6},\n",
              " {'text': 'two features so then what what are we',\n",
              "  'start': 603.48,\n",
              "  'duration': 4.32},\n",
              " {'text': 'going to do so this is going to be y =',\n",
              "  'start': 605.64,\n",
              "  'duration': 7.92},\n",
              " {'text': 'to Theta 1 X1 + Theta 2 X2 + Theta 0 so',\n",
              "  'start': 607.8,\n",
              "  'duration': 7.96},\n",
              " {'text': 'we have X1 and X2 as I showed earlier in',\n",
              "  'start': 613.56,\n",
              "  'duration': 5.399},\n",
              " {'text': 'our data set and so we we will use this',\n",
              "  'start': 615.76,\n",
              "  'duration': 5.079},\n",
              " {'text': 'equation so basically what does this',\n",
              "  'start': 618.959,\n",
              "  'duration': 4.481},\n",
              " {'text': 'equation mean and how can we write this',\n",
              "  'start': 620.839,\n",
              "  'duration': 4.761},\n",
              " {'text': \"equation so I'll just play a short video\",\n",
              "  'start': 623.44,\n",
              "  'duration': 4.8},\n",
              " {'text': \"here where I've talked about how we can\",\n",
              "  'start': 625.6,\n",
              "  'duration': 5.56},\n",
              " {'text': 'um get this equation and write it in the',\n",
              "  'start': 628.24,\n",
              "  'duration': 6.92},\n",
              " {'text': 'form of yal theta transpose X so here it',\n",
              "  'start': 631.16,\n",
              "  'duration': 6.919},\n",
              " {'text': \"starts it's just a few seconds video so\",\n",
              "  'start': 635.16,\n",
              "  'duration': 4.479},\n",
              " {'text': \"yeah so you'll understand why we are\",\n",
              "  'start': 638.079,\n",
              "  'duration': 3.32},\n",
              " {'text': 'writing y equal Theta transpose X from',\n",
              "  'start': 639.639,\n",
              "  'duration': 4.32},\n",
              " {'text': \"this equation now there's a interesting\",\n",
              "  'start': 641.399,\n",
              "  'duration': 5.88},\n",
              " {'text': 'way to write this so we want to write it',\n",
              "  'start': 643.959,\n",
              "  'duration': 5.761},\n",
              " {'text': 'in the form of a vector so that we have',\n",
              "  'start': 647.279,\n",
              "  'duration': 4.481},\n",
              " {'text': 'um simple equations to solve instead of',\n",
              "  'start': 649.72,\n",
              "  'duration': 3.6},\n",
              " {'text': 'this complicated equation with all the',\n",
              "  'start': 651.76,\n",
              "  'duration': 5.48},\n",
              " {'text': 'variables so now okay so now Theta how',\n",
              "  'start': 653.32,\n",
              "  'duration': 6.24},\n",
              " {'text': 'do we represent Theta so Theta would be',\n",
              "  'start': 657.24,\n",
              "  'duration': 4.56},\n",
              " {'text': 'n + one right total number of thetas',\n",
              "  'start': 659.56,\n",
              "  'duration': 5.0},\n",
              " {'text': 'will be n+ 1 because Theta 0 to Theta n',\n",
              "  'start': 661.8,\n",
              "  'duration': 6.0},\n",
              " {'text': 'for each of the X1 to xn feature so',\n",
              "  'start': 664.56,\n",
              "  'duration': 6.64},\n",
              " {'text': 'Theta Matrix would be Theta 0 Theta 1',\n",
              "  'start': 667.8,\n",
              "  'duration': 9.44},\n",
              " {'text': 'till Theta n 1 uh n + 1 CR 1 this is our',\n",
              "  'start': 671.2,\n",
              "  'duration': 7.68},\n",
              " {'text': 'Theta', 'start': 677.24, 'duration': 5.64},\n",
              " {'text': 'Matrix and our X Matrix looks like',\n",
              "  'start': 678.88,\n",
              "  'duration': 7.36},\n",
              " {'text': 'this I already said M cross', 'start': 682.88, 'duration': 6.04},\n",
              " {'text': 'n right M being the number of data',\n",
              "  'start': 686.24,\n",
              "  'duration': 4.76},\n",
              " {'text': 'points and N being the number of',\n",
              "  'start': 688.92,\n",
              "  'duration': 7.96},\n",
              " {'text': 'features so now our y would be', 'start': 691.0, 'duration': 10.04},\n",
              " {'text': 'M so now if you can see carefully so um',\n",
              "  'start': 696.88,\n",
              "  'duration': 7.24},\n",
              " {'text': 'so Theta 0 has one this is very',\n",
              "  'start': 701.04,\n",
              "  'duration': 4.52},\n",
              " {'text': 'important try to understand this this is',\n",
              "  'start': 704.12,\n",
              "  'duration': 2.92},\n",
              " {'text': 'the trick that you will use in most of',\n",
              "  'start': 705.56,\n",
              "  'duration': 2.839},\n",
              " {'text': \"the algorithms when you're trying to\",\n",
              "  'start': 707.04,\n",
              "  'duration': 3.479},\n",
              " {'text': 'find the uh any and solve using the',\n",
              "  'start': 708.399,\n",
              "  'duration': 4.481},\n",
              " {'text': 'vector formula so this is the one that',\n",
              "  'start': 710.519,\n",
              "  'duration': 4.361},\n",
              " {'text': \"we have so let's say if we had x z it\",\n",
              "  'start': 712.88,\n",
              "  'duration': 3.88},\n",
              " {'text': \"would have been one so what we'll do is\",\n",
              "  'start': 714.88,\n",
              "  'duration': 4.84},\n",
              " {'text': \"We'll add a column here of ones so then\",\n",
              "  'start': 716.76,\n",
              "  'duration': 6.92},\n",
              " {'text': \"we'll we'll make this let's make\",\n",
              "  'start': 719.72,\n",
              "  'duration': 3.96},\n",
              " {'text': 'this so this would be these are the nend',\n",
              "  'start': 724.56,\n",
              "  'duration': 5.68},\n",
              " {'text': \"features we have now we'll have another\",\n",
              "  'start': 727.519,\n",
              "  'duration': 6.841},\n",
              " {'text': 'column of Wes so this becomes + one so',\n",
              "  'start': 730.24,\n",
              "  'duration': 6.48},\n",
              " {'text': 'this becomes n plus one so now you can',\n",
              "  'start': 734.36,\n",
              "  'duration': 4.719},\n",
              " {'text': 'see this is M cross1 this is M CR n + 1',\n",
              "  'start': 736.72,\n",
              "  'duration': 4.919},\n",
              " {'text': 'n+ 1 CR 1 so we have a matrix',\n",
              "  'start': 739.079,\n",
              "  'duration': 5.681},\n",
              " {'text': 'multiplication here so if we have Theta',\n",
              "  'start': 741.639,\n",
              "  'duration': 5.281},\n",
              " {'text': 'transpose what would Theta transpose be',\n",
              "  'start': 744.76,\n",
              "  'duration': 6.4},\n",
              " {'text': 'like 1 cross n + 1', 'start': 746.92, 'duration': 4.24},\n",
              " {'text': \"and then we'll have X so we can write X\",\n",
              "  'start': 752.24,\n",
              "  'duration': 5.88},\n",
              " {'text': 'in the form of like n+ one cross M as',\n",
              "  'start': 755.959,\n",
              "  'duration': 3.961},\n",
              " {'text': 'well so that just depends on how you',\n",
              "  'start': 758.12,\n",
              "  'duration': 3.24},\n",
              " {'text': 'express', 'start': 759.92, 'duration': 5.76},\n",
              " {'text': \"X let's do that suppose we want to write\",\n",
              "  'start': 761.36,\n",
              "  'duration': 9.8},\n",
              " {'text': 'X in the form of um n + 1 cross',\n",
              "  'start': 765.68,\n",
              "  'duration': 5.48},\n",
              " {'text': 'l n + 1 cross M so this is our final x',\n",
              "  'start': 771.44,\n",
              "  'duration': 9.24},\n",
              " {'text': 'uh coordinates so this would be n + 1',\n",
              "  'start': 777.0,\n",
              "  'duration': 7.24},\n",
              " {'text': 'cross M now we have 1 cross M which is',\n",
              "  'start': 780.68,\n",
              "  'duration': 6.159},\n",
              " {'text': 'our y so we can say y transpose or we',\n",
              "  'start': 784.24,\n",
              "  'duration': 5.32},\n",
              " {'text': 'can represent it in the same way as X so',\n",
              "  'start': 786.839,\n",
              "  'duration': 4.68},\n",
              " {'text': 'in that case y would be 1 cross n so it',\n",
              "  'start': 789.56,\n",
              "  'duration': 3.12},\n",
              " {'text': \"doesn't really matter when you're trying\",\n",
              "  'start': 791.519,\n",
              "  'duration': 3.44},\n",
              " {'text': 'to solve this is just the way we choose',\n",
              "  'start': 792.68,\n",
              "  'duration': 4.8},\n",
              " {'text': 'to express this formula and we choose to',\n",
              "  'start': 794.959,\n",
              "  'duration': 5.961},\n",
              " {'text': 'relate uh y and x and Theta so this',\n",
              "  'start': 797.48,\n",
              "  'duration': 7.52},\n",
              " {'text': 'would be 1 cross M so now you have this',\n",
              "  'start': 800.92,\n",
              "  'duration': 7.159},\n",
              " {'text': '1 cross M so you have this equation',\n",
              "  'start': 805.0,\n",
              "  'duration': 5.04},\n",
              " {'text': 'which which is like very common y equal',\n",
              "  'start': 808.079,\n",
              "  'duration': 4.401},\n",
              " {'text': 'theeta transpose X so now that we know',\n",
              "  'start': 810.04,\n",
              "  'duration': 5.52},\n",
              " {'text': 'this we know that here we are going to',\n",
              "  'start': 812.48,\n",
              "  'duration': 5.88},\n",
              " {'text': 'have since this is the probability Yi',\n",
              "  'start': 815.56,\n",
              "  'duration': 5.24},\n",
              " {'text': 'given XI parameterized by Theta so this',\n",
              "  'start': 818.36,\n",
              "  'duration': 5.8},\n",
              " {'text': 'is going to be Theta transpose', 'start': 820.8, 'duration': 6.159},\n",
              " {'text': 'XI because Yi is one that we focusing',\n",
              "  'start': 824.16,\n",
              "  'duration': 5.16},\n",
              " {'text': 'right now that is why this is the',\n",
              "  'start': 826.959,\n",
              "  'duration': 4.8},\n",
              " {'text': 'positive distance yeah so this is the',\n",
              "  'start': 829.32,\n",
              "  'duration': 4.959},\n",
              " {'text': 'positive distance that we are using Y 1',\n",
              "  'start': 831.759,\n",
              "  'duration': 5.601},\n",
              " {'text': '+ e^ minus Theta transpose XI why are we',\n",
              "  'start': 834.279,\n",
              "  'duration': 4.48},\n",
              " {'text': 'doing that so basically what we have',\n",
              "  'start': 837.36,\n",
              "  'duration': 3.44},\n",
              " {'text': 'done means if you remember di we have',\n",
              "  'start': 838.759,\n",
              "  'duration': 4.841},\n",
              " {'text': 'written that as Theta transpose x',\n",
              "  'start': 840.8,\n",
              "  'duration': 4.839},\n",
              " {'text': \"i we don't want to get in this\", 'start': 843.6, 'duration': 3.52},\n",
              " {'text': 'perpendicular thing so we are just going',\n",
              "  'start': 845.639,\n",
              "  'duration': 4.2},\n",
              " {'text': 'to take theeta transpose XI and then',\n",
              "  'start': 847.12,\n",
              "  'duration': 4.6},\n",
              " {'text': 'which is kind of like removing all the',\n",
              "  'start': 849.839,\n",
              "  'duration': 5.161},\n",
              " {'text': 'constants Yi so y equal 1 given XI',\n",
              "  'start': 851.72,\n",
              "  'duration': 5.6},\n",
              " {'text': 'parameterized by Theta is 1 by 1 e^',\n",
              "  'start': 855.0,\n",
              "  'duration': 4.44},\n",
              " {'text': 'minus Theta transpose XI which is our',\n",
              "  'start': 857.32,\n",
              "  'duration': 3.92},\n",
              " {'text': 'distance positive', 'start': 859.44, 'duration': 5.0},\n",
              " {'text': 'distance great now the next thing is you',\n",
              "  'start': 861.24,\n",
              "  'duration': 5.12},\n",
              " {'text': 'can already guess what this is going to',\n",
              "  'start': 864.44,\n",
              "  'duration': 3.0},\n",
              " {'text': 'look', 'start': 866.36, 'duration': 4.599},\n",
              " {'text': 'like 1 by one 1 + e the power minus of',\n",
              "  'start': 867.44,\n",
              "  'duration': 5.68},\n",
              " {'text': 'minus Theta transpose', 'start': 870.959, 'duration': 5.081},\n",
              " {'text': 'X basically this is G of minus Theta',\n",
              "  'start': 873.12,\n",
              "  'duration': 6.2},\n",
              " {'text': 'transpose XI and this is this was G of',\n",
              "  'start': 876.04,\n",
              "  'duration': 5.96},\n",
              " {'text': 'theta transpose x side remember G which',\n",
              "  'start': 879.32,\n",
              "  'duration': 5.36},\n",
              " {'text': 'is the sigmoid function minus so this is',\n",
              "  'start': 882.0,\n",
              "  'duration': 5.399},\n",
              " {'text': 'going to be 1 by 1 + e to^ Theta',\n",
              "  'start': 884.68,\n",
              "  'duration': 6.48},\n",
              " {'text': 'transpose X great so now we have this',\n",
              "  'start': 887.399,\n",
              "  'duration': 5.761},\n",
              " {'text': \"out of the way it's very important to\",\n",
              "  'start': 891.16,\n",
              "  'duration': 3.88},\n",
              " {'text': 'get the intuition for this because once',\n",
              "  'start': 893.16,\n",
              "  'duration': 4.479},\n",
              " {'text': 'you understand this U 90% of the problem',\n",
              "  'start': 895.04,\n",
              "  'duration': 4.68},\n",
              " {'text': 'is done', 'start': 897.639, 'duration': 4.76},\n",
              " {'text': 'so yeah so we are 90%', 'start': 899.72, 'duration': 5.64},\n",
              " {'text': 'done so now comes the gradient descent',\n",
              "  'start': 902.399,\n",
              "  'duration': 5.0},\n",
              " {'text': 'uh function and how we are going to use',\n",
              "  'start': 905.36,\n",
              "  'duration': 3.599},\n",
              " {'text': 'this log likelihood function that we',\n",
              "  'start': 907.399,\n",
              "  'duration': 4.041},\n",
              " {'text': 'just now found out um in order to find',\n",
              "  'start': 908.959,\n",
              "  'duration': 4.841},\n",
              " {'text': 'out the parameters that is Theta 1 and',\n",
              "  'start': 911.44,\n",
              "  'duration': 4.6},\n",
              " {'text': 'Theta 2 that is Theta basically how we',\n",
              "  'start': 913.8,\n",
              "  'duration': 3.839},\n",
              " {'text': \"are going to use that so we'll be using\",\n",
              "  'start': 916.04,\n",
              "  'duration': 4.039},\n",
              " {'text': 'radiant descent obviously that is the',\n",
              "  'start': 917.639,\n",
              "  'duration': 4.961},\n",
              " {'text': 'only way here so now we need to',\n",
              "  'start': 920.079,\n",
              "  'duration': 5.12},\n",
              " {'text': 'formalize that so um basically', 'start': 922.6, 'duration': 5.679},\n",
              " {'text': 'maximizing log likelihood is Sim is same',\n",
              "  'start': 925.199,\n",
              "  'duration': 5.44},\n",
              " {'text': 'as minimizing the loss function so I',\n",
              "  'start': 928.279,\n",
              "  'duration': 3.721},\n",
              " {'text': \"hope you've seen the video where I've\",\n",
              "  'start': 930.639,\n",
              "  'duration': 4.041},\n",
              " {'text': \"explained why this is the case and I'll\",\n",
              "  'start': 932.0,\n",
              "  'duration': 4.48},\n",
              " {'text': 'reiterate it if you do not know this',\n",
              "  'start': 934.68,\n",
              "  'duration': 3.2},\n",
              " {'text': \"then it's important to know this before\",\n",
              "  'start': 936.48,\n",
              "  'duration': 4.279},\n",
              " {'text': \"you proceed okay so I I'm going to\",\n",
              "  'start': 937.88,\n",
              "  'duration': 4.84},\n",
              " {'text': 'assume that you know this so basically',\n",
              "  'start': 940.759,\n",
              "  'duration': 3.601},\n",
              " {'text': 'maximizing the log likelihood is the',\n",
              "  'start': 942.72,\n",
              "  'duration': 4.52},\n",
              " {'text': 'same as minimizing the loss function so',\n",
              "  'start': 944.36,\n",
              "  'duration': 4.56},\n",
              " {'text': 'now we are going to write a function for',\n",
              "  'start': 947.24,\n",
              "  'duration': 3.44},\n",
              " {'text': 'the log likelihood so what we had',\n",
              "  'start': 948.92,\n",
              "  'duration': 3.88},\n",
              " {'text': 'earlier was likelihood and of two',\n",
              "  'start': 950.68,\n",
              "  'duration': 3.76},\n",
              " {'text': 'distinct cases now we want to combine',\n",
              "  'start': 952.8,\n",
              "  'duration': 4.279},\n",
              " {'text': 'them together so this combination trying',\n",
              "  'start': 954.44,\n",
              "  'duration': 4.24},\n",
              " {'text': 'to understand this is very very very',\n",
              "  'start': 957.079,\n",
              "  'duration': 3.481},\n",
              " {'text': 'important important I cannot stress it',\n",
              "  'start': 958.68,\n",
              "  'duration': 3.839},\n",
              " {'text': \"enough okay so I'm going to use the\",\n",
              "  'start': 960.56,\n",
              "  'duration': 4.399},\n",
              " {'text': 'formula directly', 'start': 962.519, 'duration': 6.041},\n",
              " {'text': 'here is not much the previous uh page',\n",
              "  'start': 964.959,\n",
              "  'duration': 5.36},\n",
              " {'text': 'where I was discussing it was the',\n",
              "  'start': 968.56,\n",
              "  'duration': 3.8},\n",
              " {'text': 'likelihood function and now this is the',\n",
              "  'start': 970.319,\n",
              "  'duration': 5.361},\n",
              " {'text': 'log likelihood just using a', 'start': 972.36, 'duration': 3.32},\n",
              " {'text': 'log this the function summation I = 1 to',\n",
              "  'start': 976.04,\n",
              "  'duration': 6.279},\n",
              " {'text': 'M log of probability of y i given x i',\n",
              "  'start': 979.279,\n",
              "  'duration': 4.641},\n",
              " {'text': 'parameterized by', 'start': 982.319, 'duration': 4.721},\n",
              " {'text': 'Theta great so now we are going to break',\n",
              "  'start': 983.92,\n",
              "  'duration': 6.08},\n",
              " {'text': 'this down such that we can accommod two',\n",
              "  'start': 987.04,\n",
              "  'duration': 6.08},\n",
              " {'text': 'cases listen very', 'start': 990.0, 'duration': 5.839},\n",
              " {'text': 'carefully so this is the first one so',\n",
              "  'start': 993.12,\n",
              "  'duration': 5.279},\n",
              " {'text': 'when y = to 1 we want to take the case',\n",
              "  'start': 995.839,\n",
              "  'duration': 4.201},\n",
              " {'text': 'when y equal to', 'start': 998.399, 'duration': 4.521},\n",
              " {'text': '1 here so basically our data set is',\n",
              "  'start': 1000.04,\n",
              "  'duration': 4.88},\n",
              " {'text': 'divided into points such that y equal 1',\n",
              "  'start': 1002.92,\n",
              "  'duration': 4.8},\n",
              " {'text': 'and some have y equal 0 with some some',\n",
              "  'start': 1004.92,\n",
              "  'duration': 6.0},\n",
              " {'text': 'points X1 and X2 so we want to find out',\n",
              "  'start': 1007.72,\n",
              "  'duration': 5.4},\n",
              " {'text': 'those points which have yal 1 and take',\n",
              "  'start': 1010.92,\n",
              "  'duration': 4.44},\n",
              " {'text': 'them here in the first part of the',\n",
              "  'start': 1013.12,\n",
              "  'duration': 4.56},\n",
              " {'text': 'function so how are we going to indicate',\n",
              "  'start': 1015.36,\n",
              "  'duration': 4.44},\n",
              " {'text': 'that we are going to', 'start': 1017.68, 'duration': 4.36},\n",
              " {'text': 'use we are going to use something called',\n",
              "  'start': 1019.8,\n",
              "  'duration': 4.92},\n",
              " {'text': 'an indicator', 'start': 1022.04, 'duration': 2.68},\n",
              " {'text': 'variable so I is basically so let me',\n",
              "  'start': 1026.919,\n",
              "  'duration': 6.721},\n",
              " {'text': 'just go into this a little um in detail',\n",
              "  'start': 1030.039,\n",
              "  'duration': 5.04},\n",
              " {'text': 'so basically what is an indicator',\n",
              "  'start': 1033.64,\n",
              "  'duration': 3.48},\n",
              " {'text': 'variable so suppose you have a list of',\n",
              "  'start': 1035.079,\n",
              "  'duration': 5.76},\n",
              " {'text': 'functions suppose you have y here so 1 1',\n",
              "  'start': 1037.12,\n",
              "  'duration': 6.88},\n",
              " {'text': \"1 0 0 let's say this is our Y data set\",\n",
              "  'start': 1040.839,\n",
              "  'duration': 5.0},\n",
              " {'text': \"and we're using the indicator variable\",\n",
              "  'start': 1044.0,\n",
              "  'duration': 2.88},\n",
              " {'text': 'so if we', 'start': 1045.839, 'duration': 6.601},\n",
              " {'text': 'do here one of y i equal 1 this means',\n",
              "  'start': 1046.88,\n",
              "  'duration': 8.28},\n",
              " {'text': 'that um this means that whether Y is',\n",
              "  'start': 1052.44,\n",
              "  'duration': 5.16},\n",
              " {'text': 'equal to 1 or not so this is true so',\n",
              "  'start': 1055.16,\n",
              "  'duration': 6.28},\n",
              " {'text': 'this will take the value', 'start': 1057.6, 'duration': 3.84},\n",
              " {'text': 'one here also it will take the value one',\n",
              "  'start': 1061.679,\n",
              "  'duration': 4.201},\n",
              " {'text': 'here also it will take the value of one',\n",
              "  'start': 1064.08,\n",
              "  'duration': 5.599},\n",
              " {'text': 'now here is y1 y i equal to one no so',\n",
              "  'start': 1065.88,\n",
              "  'duration': 5.24},\n",
              " {'text': 'this will take the value zero and this',\n",
              "  'start': 1069.679,\n",
              "  'duration': 4.24},\n",
              " {'text': 'will take the value zero so do it with',\n",
              "  'start': 1071.12,\n",
              "  'duration': 7.08},\n",
              " {'text': 'me so if y i = to 1 I equal 1 2 there',\n",
              "  'start': 1073.919,\n",
              "  'duration': 6.24},\n",
              " {'text': 'are five points point so then the answer',\n",
              "  'start': 1078.2,\n",
              "  'duration': 5.839},\n",
              " {'text': 'would be what 1 + 1 + 1+ 0 + 0 which is',\n",
              "  'start': 1080.159,\n",
              "  'duration': 6.681},\n",
              " {'text': 'going to be three so this is',\n",
              "  'start': 1084.039,\n",
              "  'duration': 6.441},\n",
              " {'text': 'the this is the basic Funda of um',\n",
              "  'start': 1086.84,\n",
              "  'duration': 5.719},\n",
              " {'text': 'indicator variable so now that we know',\n",
              "  'start': 1090.48,\n",
              "  'duration': 4.52},\n",
              " {'text': 'indicator', 'start': 1092.559, 'duration': 2.441},\n",
              " {'text': 'variable so now that we know indicator',\n",
              "  'start': 1095.44,\n",
              "  'duration': 5.0},\n",
              " {'text': 'variable we can move on to the next part',\n",
              "  'start': 1097.96,\n",
              "  'duration': 4.32},\n",
              " {'text': 'of this function so what is going to',\n",
              "  'start': 1100.44,\n",
              "  'duration': 5.08},\n",
              " {'text': 'look like now you can guess it I think',\n",
              "  'start': 1102.28,\n",
              "  'duration': 7.08},\n",
              " {'text': 'you can guess it Yi equal to0 given x i',\n",
              "  'start': 1105.52,\n",
              "  'duration': 6.08},\n",
              " {'text': 'parameterized by Theta this would be',\n",
              "  'start': 1109.36,\n",
              "  'duration': 5.04},\n",
              " {'text': 'indicator variable of Yi equal to',\n",
              "  'start': 1111.6,\n",
              "  'duration': 5.8},\n",
              " {'text': 'Z great so whenever Yi is one we are',\n",
              "  'start': 1114.4,\n",
              "  'duration': 5.2},\n",
              " {'text': 'going to use this because probability of',\n",
              "  'start': 1117.4,\n",
              "  'duration': 5.72},\n",
              " {'text': 'Yi being one parameterized by Theta',\n",
              "  'start': 1119.6,\n",
              "  'duration': 6.12},\n",
              " {'text': \"given x i and whenever Yi is zero we're\",\n",
              "  'start': 1123.12,\n",
              "  'duration': 4.559},\n",
              " {'text': 'going to use this so now this log',\n",
              "  'start': 1125.72,\n",
              "  'duration': 4.199},\n",
              " {'text': 'likelihood can be expressed like this',\n",
              "  'start': 1127.679,\n",
              "  'duration': 5.441},\n",
              " {'text': 'very easily now the work is almost done',\n",
              "  'start': 1129.919,\n",
              "  'duration': 6.281},\n",
              " {'text': \"I can say it's 95% done so now what we\",\n",
              "  'start': 1133.12,\n",
              "  'duration': 5.08},\n",
              " {'text': 'want to do is just put in this value we',\n",
              "  'start': 1136.2,\n",
              "  'duration': 3.64},\n",
              " {'text': 'know this value', 'start': 1138.2, 'duration': 6.44},\n",
              " {'text': 'so let me just um let me just clean this',\n",
              "  'start': 1139.84,\n",
              "  'duration': 7.36},\n",
              " {'text': 'up okay so now what we going to do is do',\n",
              "  'start': 1144.64,\n",
              "  'duration': 4.6},\n",
              " {'text': 'the substitution and then differentiate',\n",
              "  'start': 1147.2,\n",
              "  'duration': 4.76},\n",
              " {'text': 'that and equate it to zero right how we',\n",
              "  'start': 1149.24,\n",
              "  'duration': 3.76},\n",
              " {'text': 'know this', 'start': 1151.96, 'duration': 5.959},\n",
              " {'text': 'one probability of y i = 1 a given x i',\n",
              "  'start': 1153.0,\n",
              "  'duration': 8.919},\n",
              " {'text': 'parameterized by Theta 1 by 1 + e^ minus',\n",
              "  'start': 1157.919,\n",
              "  'duration': 8.161},\n",
              " {'text': 'Theta transpose X I just nowar wrote',\n",
              "  'start': 1161.919,\n",
              "  'duration': 7.88},\n",
              " {'text': 'it and the other one is probability of y',\n",
              "  'start': 1166.08,\n",
              "  'duration': 7.32},\n",
              " {'text': 'i = 0 given x i parameterized by Theta',\n",
              "  'start': 1169.799,\n",
              "  'duration': 8.401},\n",
              " {'text': 'is 1 by 1 + e^ theeta transal x',\n",
              "  'start': 1173.4,\n",
              "  'duration': 7.759},\n",
              " {'text': 'side okay so now what we going to do is',\n",
              "  'start': 1178.2,\n",
              "  'duration': 6.839},\n",
              " {'text': 'substitute so this this', 'start': 1181.159, 'duration': 6.081},\n",
              " {'text': 'function would look something like this',\n",
              "  'start': 1185.039,\n",
              "  'duration': 5.401},\n",
              " {'text': 'this would be Yi by Yi itself because',\n",
              "  'start': 1187.24,\n",
              "  'duration': 4.64},\n",
              " {'text': 'you saw that when I was showing that',\n",
              "  'start': 1190.44,\n",
              "  'duration': 4.52},\n",
              " {'text': 'graph so if Yi is one then this will be',\n",
              "  'start': 1191.88,\n",
              "  'duration': 4.84},\n",
              " {'text': 'one right and if it Y is zero this will',\n",
              "  'start': 1194.96,\n",
              "  'duration': 4.4},\n",
              " {'text': 'be zero so we can just take Yi',\n",
              "  'start': 1196.72,\n",
              "  'duration': 7.199},\n",
              " {'text': 'and then put in the function here 1 by',\n",
              "  'start': 1199.36,\n",
              "  'duration': 4.559},\n",
              " {'text': '1 and for this one we know what is going',\n",
              "  'start': 1204.679,\n",
              "  'duration': 6.681},\n",
              " {'text': 'to be you can guess it 1 minus Yi so if',\n",
              "  'start': 1207.679,\n",
              "  'duration': 6.161},\n",
              " {'text': 'Y is 1 this will be zero which is what',\n",
              "  'start': 1211.36,\n",
              "  'duration': 4.679},\n",
              " {'text': 'we want if Y is zero this would be one',\n",
              "  'start': 1213.84,\n",
              "  'duration': 5.44},\n",
              " {'text': 'which is exactly what we want perfect so',\n",
              "  'start': 1216.039,\n",
              "  'duration': 6.241},\n",
              " {'text': 'now we have 1 -', 'start': 1219.28, 'duration': 9.48},\n",
              " {'text': 'Yi log of 1 by 1 + e^ th transpose x i',\n",
              "  'start': 1222.28,\n",
              "  'duration': 10.32},\n",
              " {'text': 'now we have this great so now as soon as',\n",
              "  'start': 1228.76,\n",
              "  'duration': 7.6},\n",
              " {'text': \"we have this our work is done so it's\",\n",
              "  'start': 1232.6,\n",
              "  'duration': 5.88},\n",
              " {'text': 'now 100% done so we just need to',\n",
              "  'start': 1236.36,\n",
              "  'duration': 4.08},\n",
              " {'text': 'differentiate this and equate it to zero',\n",
              "  'start': 1238.48,\n",
              "  'duration': 3.92},\n",
              " {'text': \"and then we'll have an equation for our\",\n",
              "  'start': 1240.44,\n",
              "  'duration': 4.52},\n",
              " {'text': 'thetas and we can just use', 'start': 1242.4, 'duration': 5.88},\n",
              " {'text': 'that um we can use it like in gradient',\n",
              "  'start': 1244.96,\n",
              "  'duration': 5.16},\n",
              " {'text': \"descent as you know so I I'll just come\",\n",
              "  'start': 1248.28,\n",
              "  'duration': 2.56},\n",
              " {'text': 'to', 'start': 1250.12, 'duration': 3.52},\n",
              " {'text': \"that so first let's simplify this before\",\n",
              "  'start': 1250.84,\n",
              "  'duration': 5.92},\n",
              " {'text': 'we differentiate', 'start': 1253.64, 'duration': 3.12},\n",
              " {'text': \"this minus is because I'm putting it\",\n",
              "  'start': 1262.559,\n",
              "  'duration': 9.081},\n",
              " {'text': 'here and this would be um this would',\n",
              "  'start': 1267.36,\n",
              "  'duration': 7.84},\n",
              " {'text': 'have a minus already so we can',\n",
              "  'start': 1271.64,\n",
              "  'duration': 3.56},\n",
              " {'text': 'just y i-', 'start': 1275.72, 'duration': 4.24},\n",
              " {'text': '1', 'start': 1282.919, 'duration': 5.401},\n",
              " {'text': \"cool um yeah so now let's remember this\",\n",
              "  'start': 1284.84,\n",
              "  'duration': 5.68},\n",
              " {'text': 'for', 'start': 1288.32, 'duration': 2.2},\n",
              " {'text': 'fora now that we have this formula then',\n",
              "  'start': 1292.159,\n",
              "  'duration': 6.081},\n",
              " {'text': \"let's differentiate\", 'start': 1295.0, 'duration': 3.24},\n",
              " {'text': \"that Del L equal to Z I'm using Delta\",\n",
              "  'start': 1299.64,\n",
              "  'duration': 8.399},\n",
              " {'text': 'again because um we have multiple',\n",
              "  'start': 1303.559,\n",
              "  'duration': 6.881},\n",
              " {'text': \"Dimensions um okay so now we'll\",\n",
              "  'start': 1308.039,\n",
              "  'duration': 4.401},\n",
              " {'text': 'differentiate this one and equate it to',\n",
              "  'start': 1310.44,\n",
              "  'duration': 4.479},\n",
              " {'text': \"zero so let's differentiate we have this\",\n",
              "  'start': 1312.44,\n",
              "  'duration': 5.4},\n",
              " {'text': 'equation this is basically our LL Theta',\n",
              "  'start': 1314.919,\n",
              "  'duration': 4.801},\n",
              " {'text': 'and this moment and we now want to',\n",
              "  'start': 1317.84,\n",
              "  'duration': 4.64},\n",
              " {'text': \"differentiate this so let's do that so\",\n",
              "  'start': 1319.72,\n",
              "  'duration': 6.6},\n",
              " {'text': 'summation I = 1 to M - y i we', 'start': 1322.48, 'duration': 6.16},\n",
              " {'text': 'differentiating with respect to Theta so',\n",
              "  'start': 1326.32,\n",
              "  'duration': 3.52},\n",
              " {'text': 'this would be this would get', 'start': 1328.64, 'duration': 3.8},\n",
              " {'text': 'differentiated so as we know', 'start': 1329.84, 'duration': 5.319},\n",
              " {'text': 'differentiation of log', 'start': 1332.44, 'duration': 4.8},\n",
              " {'text': 'X differentiation what is', 'start': 1335.159, 'duration': 4.481},\n",
              " {'text': \"differentiation of log X it's one one\",\n",
              "  'start': 1337.24,\n",
              "  'duration': 6.72},\n",
              " {'text': 'upon X right so DDX of log X is 1x X so',\n",
              "  'start': 1339.64,\n",
              "  'duration': 6.76},\n",
              " {'text': 'that is what we trying to use here and',\n",
              "  'start': 1343.96,\n",
              "  'duration': 4.12},\n",
              " {'text': \"if and one more thing we're trying to\",\n",
              "  'start': 1346.4,\n",
              "  'duration': 5.56},\n",
              " {'text': 'use is DDX of so f of x then it is going',\n",
              "  'start': 1348.08,\n",
              "  'duration': 5.8},\n",
              " {'text': 'to be DF', 'start': 1351.96, 'duration': 4.24},\n",
              " {'text': 'DX upon', 'start': 1353.88, 'duration': 4.679},\n",
              " {'text': 'X this is going to go down and then this',\n",
              "  'start': 1356.2,\n",
              "  'duration': 5.599},\n",
              " {'text': \"will be DF upon DX so let's use that so\",\n",
              "  'start': 1358.559,\n",
              "  'duration': 4.921},\n",
              " {'text': \"we put this down and now we're going to\",\n",
              "  'start': 1361.799,\n",
              "  'duration': 5.321},\n",
              " {'text': 'differentiate this with respect to um',\n",
              "  'start': 1363.48,\n",
              "  'duration': 6.04},\n",
              " {'text': 'with respect to Theta yeah so with',\n",
              "  'start': 1367.12,\n",
              "  'duration': 3.679},\n",
              " {'text': 'respect to Theta what would that look',\n",
              "  'start': 1369.52,\n",
              "  'duration': 3.2},\n",
              " {'text': 'like this would be zero and this would',\n",
              "  'start': 1370.799,\n",
              "  'duration': 5.641},\n",
              " {'text': 'be e^ minus Theta transpose x i and this',\n",
              "  'start': 1372.72,\n",
              "  'duration': 6.88},\n",
              " {'text': 'would be- XI', 'start': 1376.44, 'duration': 6.52},\n",
              " {'text': 'correct yeah got it so this one this one',\n",
              "  'start': 1379.6,\n",
              "  'duration': 5.679},\n",
              " {'text': 'done now the next one next one so what',\n",
              "  'start': 1382.96,\n",
              "  'duration': 5.64},\n",
              " {'text': \"would this look like let's see so Yus\",\n",
              "  'start': 1385.279,\n",
              "  'duration': 6.921},\n",
              " {'text': 'one would be in that way keep that oh',\n",
              "  'start': 1388.6,\n",
              "  'duration': 6.319},\n",
              " {'text': 'the same thing so the below it will go e',\n",
              "  'start': 1392.2,\n",
              "  'duration': 6.44},\n",
              " {'text': 'to the Theta transpose XI and here it',\n",
              "  'start': 1394.919,\n",
              "  'duration': 6.801},\n",
              " {'text': 'will be Theta transpose XI multipli with',\n",
              "  'start': 1398.64,\n",
              "  'duration': 6.279},\n",
              " {'text': 'X I so this is the way it goes and then',\n",
              "  'start': 1401.72,\n",
              "  'duration': 4.92},\n",
              " {'text': \"we're differentiating this one so e to\",\n",
              "  'start': 1404.919,\n",
              "  'duration': 3.88},\n",
              " {'text': 'the^ X differentiation is e to the^ X X',\n",
              "  'start': 1406.64,\n",
              "  'duration': 6.36},\n",
              " {'text': 'if you know DDX of e^ X is e to^ x that',\n",
              "  'start': 1408.799,\n",
              "  'duration': 6.36},\n",
              " {'text': \"we're trying to use here so e to the^ X\",\n",
              "  'start': 1413.0,\n",
              "  'duration': 4.12},\n",
              " {'text': 'differentiation would be this and then',\n",
              "  'start': 1415.159,\n",
              "  'duration': 4.161},\n",
              " {'text': 'differentiating this with respect to',\n",
              "  'start': 1417.12,\n",
              "  'duration': 3.439},\n",
              " {'text': 'Theta would be', 'start': 1419.32, 'duration': 3.8},\n",
              " {'text': \"XI great so we now I've got both of\",\n",
              "  'start': 1420.559,\n",
              "  'duration': 4.48},\n",
              " {'text': 'these out of the way this would get plus',\n",
              "  'start': 1423.12,\n",
              "  'duration': 6.24},\n",
              " {'text': 'because of two minuses now um yeah so',\n",
              "  'start': 1425.039,\n",
              "  'duration': 6.481},\n",
              " {'text': 'now if you want see so this term and',\n",
              "  'start': 1429.36,\n",
              "  'duration': 4.12},\n",
              " {'text': \"this term is similar so we're going to\",\n",
              "  'start': 1431.52,\n",
              "  'duration': 3.399},\n",
              " {'text': 'glove them', 'start': 1433.48, 'duration': 6.28},\n",
              " {'text': 'together now we are going to use a trick',\n",
              "  'start': 1434.919,\n",
              "  'duration': 7.201},\n",
              " {'text': \"listen very carefully otherwise you'll\",\n",
              "  'start': 1439.76,\n",
              "  'duration': 5.2},\n",
              " {'text': \"miss it listen very carefully I'm going\",\n",
              "  'start': 1442.12,\n",
              "  'duration': 5.039},\n",
              " {'text': 'to use a very simple trick very old',\n",
              "  'start': 1444.96,\n",
              "  'duration': 3.64},\n",
              " {'text': \"trick in the dictionary but I'm going to\",\n",
              "  'start': 1447.159,\n",
              "  'duration': 4.441},\n",
              " {'text': 'use that so if you see here we have what',\n",
              "  'start': 1448.6,\n",
              "  'duration': 5.959},\n",
              " {'text': 'do we have e to the^ minus x 1 plus e to',\n",
              "  'start': 1451.6,\n",
              "  'duration': 8.24},\n",
              " {'text': \"the power - x right so now if I let's do\",\n",
              "  'start': 1454.559,\n",
              "  'duration': 7.841},\n",
              " {'text': 'that so if I convert this 1 by e to the',\n",
              "  'start': 1459.84,\n",
              "  'duration': 6.199},\n",
              " {'text': 'power x upon 1 + 1 by e to^ x now get',\n",
              "  'start': 1462.4,\n",
              "  'duration': 8.56},\n",
              " {'text': 'this up here we would have 1 by e^ Min -',\n",
              "  'start': 1466.039,\n",
              "  'duration': 8.321},\n",
              " {'text': \"x we're multiplying with e the^ Min - x\",\n",
              "  'start': 1470.96,\n",
              "  'duration': 5.719},\n",
              " {'text': 'we are multiplying e the^ x so we would',\n",
              "  'start': 1474.36,\n",
              "  'duration': 5.679},\n",
              " {'text': 'have e to^ x + 1', 'start': 1476.679, 'duration': 6.681},\n",
              " {'text': 'correct these two are same you agree so',\n",
              "  'start': 1480.039,\n",
              "  'duration': 6.12},\n",
              " {'text': 'now we are going to use the same thing',\n",
              "  'start': 1483.36,\n",
              "  'duration': 6.799},\n",
              " {'text': 'with um e^ X so e to^ x we have 1 + e to',\n",
              "  'start': 1486.159,\n",
              "  'duration': 6.601},\n",
              " {'text': 'the^ X so now we are going to do is 1 by',\n",
              "  'start': 1490.159,\n",
              "  'duration': 8.64},\n",
              " {'text': 'e ^ - x upon 1 by e ^ - x + 1 so now',\n",
              "  'start': 1492.76,\n",
              "  'duration': 8.08},\n",
              " {'text': \"what will this be let's multiply e^ Min\",\n",
              "  'start': 1498.799,\n",
              "  'duration': 5.6},\n",
              " {'text': '- x we would have 1 by 1 + e^ -',\n",
              "  'start': 1500.84,\n",
              "  'duration': 6.52},\n",
              " {'text': 'x so does this does this look familiar',\n",
              "  'start': 1504.399,\n",
              "  'duration': 5.801},\n",
              " {'text': 'so now if we convert this one like this',\n",
              "  'start': 1507.36,\n",
              "  'duration': 4.439},\n",
              " {'text': 'we have the same denominator and we can',\n",
              "  'start': 1510.2,\n",
              "  'duration': 4.24},\n",
              " {'text': 'combine these terms excellent right',\n",
              "  'start': 1511.799,\n",
              "  'duration': 4.041},\n",
              " {'text': 'remember this trick this will come in',\n",
              "  'start': 1514.44,\n",
              "  'duration': 4.28},\n",
              " {'text': 'handy later as', 'start': 1515.84, 'duration': 2.88},\n",
              " {'text': 'well great so now you just saw what I',\n",
              "  'start': 1520.24,\n",
              "  'duration': 4.679},\n",
              " {'text': \"did so I'm going to use that I'm going\",\n",
              "  'start': 1523.0,\n",
              "  'duration': 4.24},\n",
              " {'text': 'to convert this one into this form you',\n",
              "  'start': 1524.919,\n",
              "  'duration': 4.721},\n",
              " {'text': 'will soon realize why am doing this one',\n",
              "  'start': 1527.24,\n",
              "  'duration': 6.159},\n",
              " {'text': 'and not the opposite y i eus Theta',\n",
              "  'start': 1529.64,\n",
              "  'duration': 8.32},\n",
              " {'text': 'transpose x i 1 + eus Theta transpose x',\n",
              "  'start': 1533.399,\n",
              "  'duration': 8.681},\n",
              " {'text': 'i mtip with x i and this', 'start': 1537.96, 'duration': 9.959},\n",
              " {'text': \"one I = 1 to m y i - one let's do this\",\n",
              "  'start': 1542.08,\n",
              "  'duration': 8.28},\n",
              " {'text': 'one so what is this going to look like 1',\n",
              "  'start': 1547.919,\n",
              "  'duration': 8.36},\n",
              " {'text': 'by 1 + e ^ minus Theta transpose x',\n",
              "  'start': 1550.36,\n",
              "  'duration': 8.799},\n",
              " {'text': 'correct you got it this this is',\n",
              "  'start': 1556.279,\n",
              "  'duration': 5.801},\n",
              " {'text': 'extremely a very neat trick that we use',\n",
              "  'start': 1559.159,\n",
              "  'duration': 4.961},\n",
              " {'text': 'and it solves most of the problems as',\n",
              "  'start': 1562.08,\n",
              "  'duration': 4.36},\n",
              " {'text': 'you just saw so now we have the same',\n",
              "  'start': 1564.12,\n",
              "  'duration': 5.08},\n",
              " {'text': 'denominator we can combine these two',\n",
              "  'start': 1566.44,\n",
              "  'duration': 8.04},\n",
              " {'text': \"terms so um yeah so let's do\", 'start': 1569.2, 'duration': 9.0},\n",
              " {'text': \"that so let's simplify so what do we\",\n",
              "  'start': 1574.48,\n",
              "  'duration': 7.439},\n",
              " {'text': \"have here let's see we have 1 by 1 + e^\",\n",
              "  'start': 1578.2,\n",
              "  'duration': 9.199},\n",
              " {'text': 'th transpose XI and here we have Yi XI',\n",
              "  'start': 1581.919,\n",
              "  'duration': 9.48},\n",
              " {'text': 'from here e^ minus Theta transpose x i',\n",
              "  'start': 1587.399,\n",
              "  'duration': 9.081},\n",
              " {'text': 'we have plus y i x i from here and we',\n",
              "  'start': 1591.399,\n",
              "  'duration': 8.081},\n",
              " {'text': 'have minus', 'start': 1596.48, 'duration': 3.0},\n",
              " {'text': 'XI so as you can see here if you just',\n",
              "  'start': 1599.799,\n",
              "  'duration': 4.721},\n",
              " {'text': 'take yxi common we have 1 plus e to the^',\n",
              "  'start': 1602.08,\n",
              "  'duration': 5.599},\n",
              " {'text': 'minus Theta transpose x i yes we are',\n",
              "  'start': 1604.52,\n",
              "  'duration': 5.92},\n",
              " {'text': 'very very close to the formula that we',\n",
              "  'start': 1607.679,\n",
              "  'duration': 5.12},\n",
              " {'text': \"want the simplified form so let's do\",\n",
              "  'start': 1610.44,\n",
              "  'duration': 6.359},\n",
              " {'text': \"that y i x i let's take that common and\",\n",
              "  'start': 1612.799,\n",
              "  'duration': 8.281},\n",
              " {'text': 'we have 1 + e^ minus Theta transpose x i',\n",
              "  'start': 1616.799,\n",
              "  'duration': 8.401},\n",
              " {'text': 'upon 1 + e^ minus Theta transpose x i',\n",
              "  'start': 1621.08,\n",
              "  'duration': 9.04},\n",
              " {'text': 'these two get canceled and this is - x i',\n",
              "  'start': 1625.2,\n",
              "  'duration': 9.24},\n",
              " {'text': 'upon 1 + e^ minus Theta transpose x i we',\n",
              "  'start': 1630.12,\n",
              "  'duration': 5.84},\n",
              " {'text': 'are nearing the term that we trying to',\n",
              "  'start': 1634.44,\n",
              "  'duration': 4.359},\n",
              " {'text': \"find so do not forget this though it's\",\n",
              "  'start': 1635.96,\n",
              "  'duration': 3.959},\n",
              " {'text': 'very', 'start': 1638.799, 'duration': 3.441},\n",
              " {'text': 'important this derivation is extremely',\n",
              "  'start': 1639.919,\n",
              "  'duration': 5.88},\n",
              " {'text': 'important is gold pure gold so great now',\n",
              "  'start': 1642.24,\n",
              "  'duration': 5.6},\n",
              " {'text': 'that we have this so we are trying to',\n",
              "  'start': 1645.799,\n",
              "  'duration': 4.841},\n",
              " {'text': 'find out yeah yeah yeah we are almost we',\n",
              "  'start': 1647.84,\n",
              "  'duration': 6.16},\n",
              " {'text': 'are we like literally at the', 'start': 1650.64, 'duration': 3.36},\n",
              " {'text': \"end so I'll just take this one out put\",\n",
              "  'start': 1656.279,\n",
              "  'duration': 6.441},\n",
              " {'text': \"it here so this is going to be let's\",\n",
              "  'start': 1660.48,\n",
              "  'duration': 5.76},\n",
              " {'text': 'take XI out so we are going to',\n",
              "  'start': 1662.72,\n",
              "  'duration': 8.16},\n",
              " {'text': 'have y i - 1 by 1 + e^ minus Theta',\n",
              "  'start': 1666.24,\n",
              "  'duration': 8.48},\n",
              " {'text': 'transpose x i multip with x i does this',\n",
              "  'start': 1670.88,\n",
              "  'duration': 6.2},\n",
              " {'text': 'look same does this look familiar this',\n",
              "  'start': 1674.72,\n",
              "  'duration': 4.199},\n",
              " {'text': 'is what this is the sigma function',\n",
              "  'start': 1677.08,\n",
              "  'duration': 5.199},\n",
              " {'text': \"correct so let's put that in here y i\",\n",
              "  'start': 1678.919,\n",
              "  'duration': 7.64},\n",
              " {'text': 'minus G of theta transpose x i',\n",
              "  'start': 1682.279,\n",
              "  'duration': 9.921},\n",
              " {'text': 'multiplied x i summ i = 1 to n perfect',\n",
              "  'start': 1686.559,\n",
              "  'duration': 7.401},\n",
              " {'text': 'this is the formula that we are that we',\n",
              "  'start': 1692.2,\n",
              "  'duration': 3.88},\n",
              " {'text': 'were looking for now that we have this',\n",
              "  'start': 1693.96,\n",
              "  'duration': 5.199},\n",
              " {'text': 'we know what Delta Theta LL Theta looks',\n",
              "  'start': 1696.08,\n",
              "  'duration': 7.64},\n",
              " {'text': 'like and we will just now um use our',\n",
              "  'start': 1699.159,\n",
              "  'duration': 6.601},\n",
              " {'text': 'gradient dist formula so remember this',\n",
              "  'start': 1703.72,\n",
              "  'duration': 4.52},\n",
              " {'text': 'formula we have spent a lot of time and',\n",
              "  'start': 1705.76,\n",
              "  'duration': 4.24},\n",
              " {'text': 'a lot of effort and hard work to get',\n",
              "  'start': 1708.24,\n",
              "  'duration': 6.0},\n",
              " {'text': 'here so this is very very very', 'start': 1710.0, 'duration': 4.24},\n",
              " {'text': \"important and if you've got this then\",\n",
              "  'start': 1714.399,\n",
              "  'duration': 4.12},\n",
              " {'text': 'you practically understood', 'start': 1716.76, 'duration': 6.919},\n",
              " {'text': 'classification you have got it',\n",
              "  'start': 1718.519,\n",
              "  'duration': 5.16},\n",
              " {'text': \"yeah perfect now uh now let's let's do\",\n",
              "  'start': 1724.799,\n",
              "  'duration': 5.401},\n",
              " {'text': 'that so now we are going to do the',\n",
              "  'start': 1728.6,\n",
              "  'duration': 3.48},\n",
              " {'text': 'radiant desent so if you know what is',\n",
              "  'start': 1730.2,\n",
              "  'duration': 3.719},\n",
              " {'text': 'gradient descent so basically gradient',\n",
              "  'start': 1732.08,\n",
              "  'duration': 3.92},\n",
              " {'text': \"descent means uh I'll just repeat it\",\n",
              "  'start': 1733.919,\n",
              "  'duration': 4.521},\n",
              " {'text': \"like in short I have told about it I've\",\n",
              "  'start': 1736.0,\n",
              "  'duration': 4.08},\n",
              " {'text': 'spoken about it in my earlier videos but',\n",
              "  'start': 1738.44,\n",
              "  'duration': 4.079},\n",
              " {'text': \"I'll just tell about it in short so this\",\n",
              "  'start': 1740.08,\n",
              "  'duration': 4.12},\n",
              " {'text': 'is where we are starting so this is our',\n",
              "  'start': 1742.519,\n",
              "  'duration': 4.081},\n",
              " {'text': 'loss or our L log likelihood that we are',\n",
              "  'start': 1744.2,\n",
              "  'duration': 4.599},\n",
              " {'text': 'starting okay so if we not talking about',\n",
              "  'start': 1746.6,\n",
              "  'duration': 3.88},\n",
              " {'text': 'log likelihood then this would not be',\n",
              "  'start': 1748.799,\n",
              "  'duration': 4.521},\n",
              " {'text': 'the curve in that case because in log',\n",
              "  'start': 1750.48,\n",
              "  'duration': 4.319},\n",
              " {'text': \"likelihood we're trying to maximize it\",\n",
              "  'start': 1753.32,\n",
              "  'duration': 2.599},\n",
              " {'text': \"in loss function we're trying to\",\n",
              "  'start': 1754.799,\n",
              "  'duration': 2.961},\n",
              " {'text': 'minimize it so the curve would look',\n",
              "  'start': 1755.919,\n",
              "  'duration': 4.961},\n",
              " {'text': 'something like this so here we are',\n",
              "  'start': 1757.76,\n",
              "  'duration': 6.36},\n",
              " {'text': 'currently this is our log likelihood and',\n",
              "  'start': 1760.88,\n",
              "  'duration': 5.36},\n",
              " {'text': \"we're trying to reach this point which\",\n",
              "  'start': 1764.12,\n",
              "  'duration': 6.52},\n",
              " {'text': 'is our top Target which is our maximum',\n",
              "  'start': 1766.24,\n",
              "  'duration': 7.0},\n",
              " {'text': 'log likelihood function so if you have',\n",
              "  'start': 1770.64,\n",
              "  'duration': 4.32},\n",
              " {'text': \"this so this is the Target that we're\",\n",
              "  'start': 1773.24,\n",
              "  'duration': 3.08},\n",
              " {'text': 'trying to achieve so what are we going',\n",
              "  'start': 1774.96,\n",
              "  'duration': 5.04},\n",
              " {'text': 'to do simple so we are going to use our',\n",
              "  'start': 1776.32,\n",
              "  'duration': 5.64},\n",
              " {'text': 'gradient descent', 'start': 1780.0, 'duration': 4.84},\n",
              " {'text': \"formula Theta t + 1 so we'll initialize\",\n",
              "  'start': 1781.96,\n",
              "  'duration': 5.48},\n",
              " {'text': 'Theta with some random values or zero',\n",
              "  'start': 1784.84,\n",
              "  'duration': 5.36},\n",
              " {'text': \"and then we'll follow this curve yeah so\",\n",
              "  'start': 1787.44,\n",
              "  'duration': 4.2},\n",
              " {'text': 'we are going to follow this curve so',\n",
              "  'start': 1790.2,\n",
              "  'duration': 4.52},\n",
              " {'text': 'Theta t + 1 is going to be Theta T',\n",
              "  'start': 1791.64,\n",
              "  'duration': 7.919},\n",
              " {'text': 'minusa Delta Theta LL Theta', 'start': 1794.72, 'duration': 8.319},\n",
              " {'text': 'um sorry minus of this so practically',\n",
              "  'start': 1799.559,\n",
              "  'duration': 5.72},\n",
              " {'text': \"it's plus why is this not said we are\",\n",
              "  'start': 1803.039,\n",
              "  'duration': 4.201},\n",
              " {'text': 'going not in the direction of the slope',\n",
              "  'start': 1805.279,\n",
              "  'duration': 5.12},\n",
              " {'text': 'we going um we are going to maximize the',\n",
              "  'start': 1807.24,\n",
              "  'duration': 4.52},\n",
              " {'text': 'likelihood that is why we need to move',\n",
              "  'start': 1810.399,\n",
              "  'duration': 3.241},\n",
              " {'text': 'in this direction which is very',\n",
              "  'start': 1811.76,\n",
              "  'duration': 4.159},\n",
              " {'text': 'important so now this is our gradient',\n",
              "  'start': 1813.64,\n",
              "  'duration': 3.84},\n",
              " {'text': \"descent formula so I'll just write it\",\n",
              "  'start': 1815.919,\n",
              "  'duration': 2.681},\n",
              " {'text': 'out', 'start': 1817.48, 'duration': 5.16},\n",
              " {'text': 'again if this is not very', 'start': 1818.6, 'duration': 4.04},\n",
              " {'text': 'cleara what you just need to do is just',\n",
              "  'start': 1823.88,\n",
              "  'duration': 4.12},\n",
              " {'text': 'place the function that we just now',\n",
              "  'start': 1826.24,\n",
              "  'duration': 4.72},\n",
              " {'text': 'derived in here and you have it and you',\n",
              "  'start': 1828.0,\n",
              "  'duration': 4.799},\n",
              " {'text': 'can follow it for as many as you want',\n",
              "  'start': 1830.96,\n",
              "  'duration': 4.12},\n",
              " {'text': 'until you get convergence this is the',\n",
              "  'start': 1832.799,\n",
              "  'duration': 4.401},\n",
              " {'text': 'learning', 'start': 1835.08, 'duration': 4.479},\n",
              " {'text': 'rate and learning rate determines the',\n",
              "  'start': 1837.2,\n",
              "  'duration': 3.4},\n",
              " {'text': 'rate of', 'start': 1839.559, 'duration': 3.561},\n",
              " {'text': 'convergence and we have Theta at the end',\n",
              "  'start': 1840.6,\n",
              "  'duration': 5.16},\n",
              " {'text': 'of this', 'start': 1843.12, 'duration': 5.399},\n",
              " {'text': \"perfect we've literally come this far\",\n",
              "  'start': 1845.76,\n",
              "  'duration': 4.56},\n",
              " {'text': 'then you deserve a methal you are a',\n",
              "  'start': 1848.519,\n",
              "  'duration': 3.321},\n",
              " {'text': \"genius you've understood everything and\",\n",
              "  'start': 1850.32,\n",
              "  'duration': 2.839},\n",
              " {'text': 'you can tackle any problem of',\n",
              "  'start': 1851.84,\n",
              "  'duration': 2.839},\n",
              " {'text': 'classification now between in interview',\n",
              "  'start': 1853.159,\n",
              "  'duration': 3.4},\n",
              " {'text': 'in placement in job in Gate exam in',\n",
              "  'start': 1854.679,\n",
              "  'duration': 4.081},\n",
              " {'text': 'college exam anything', 'start': 1856.559, 'duration': 6.0},\n",
              " {'text': 'great great amazing so now just a simple',\n",
              "  'start': 1858.76,\n",
              "  'duration': 5.44},\n",
              " {'text': 'thing so basically now that we have',\n",
              "  'start': 1862.559,\n",
              "  'duration': 3.84},\n",
              " {'text': 'Theta we know that the equation of the',\n",
              "  'start': 1864.2,\n",
              "  'duration': 4.079},\n",
              " {'text': 'line is going to be Theta transpose X if',\n",
              "  'start': 1866.399,\n",
              "  'duration': 3.801},\n",
              " {'text': 'you remember that I was using earlier',\n",
              "  'start': 1868.279,\n",
              "  'duration': 5.0},\n",
              " {'text': \"Theta transpose x correct so now it's\",\n",
              "  'start': 1870.2,\n",
              "  'duration': 5.4},\n",
              " {'text': 'very important to find out so now we',\n",
              "  'start': 1873.279,\n",
              "  'duration': 3.52},\n",
              " {'text': 'want to find out the equation of the',\n",
              "  'start': 1875.6,\n",
              "  'duration': 3.36},\n",
              " {'text': 'line right so just remember one thing',\n",
              "  'start': 1876.799,\n",
              "  'duration': 4.801},\n",
              " {'text': 'that we did so when I was saying that',\n",
              "  'start': 1878.96,\n",
              "  'duration': 8.76},\n",
              " {'text': 'our di is Theta transpose XI right so',\n",
              "  'start': 1881.6,\n",
              "  'duration': 11.079},\n",
              " {'text': 'and and G of di or G of C transpose X I',\n",
              "  'start': 1887.72,\n",
              "  'duration': 10.199},\n",
              " {'text': 'would be half if di is zero remember',\n",
              "  'start': 1892.679,\n",
              "  'duration': 8.72},\n",
              " {'text': \"this right so let's let's get it get\",\n",
              "  'start': 1897.919,\n",
              "  'duration': 6.681},\n",
              " {'text': 'that from here so 1 by 1 + e^ minus di',\n",
              "  'start': 1901.399,\n",
              "  'duration': 5.481},\n",
              " {'text': \"is the transpose x i so I'm going to put\",\n",
              "  'start': 1904.6,\n",
              "  'duration': 5.199},\n",
              " {'text': 'this in here equal to half now solving',\n",
              "  'start': 1906.88,\n",
              "  'duration': 4.799},\n",
              " {'text': 'the equation what we have it is the',\n",
              "  'start': 1909.799,\n",
              "  'duration': 5.36},\n",
              " {'text': 'transpose e the^ minus thepose x i 1+ is',\n",
              "  'start': 1911.679,\n",
              "  'duration': 6.96},\n",
              " {'text': 'equal to 2 so this would be 1 so now',\n",
              "  'start': 1915.159,\n",
              "  'duration': 7.561},\n",
              " {'text': 'Theta transpose x i is equal to Z which',\n",
              "  'start': 1918.639,\n",
              "  'duration': 5.681},\n",
              " {'text': 'is exactly the equation of the line that',\n",
              "  'start': 1922.72,\n",
              "  'duration': 3.28},\n",
              " {'text': \"we're looking for and this will be the\",\n",
              "  'start': 1924.32,\n",
              "  'duration': 4.239},\n",
              " {'text': 'decision boundary you remember this one',\n",
              "  'start': 1926.0,\n",
              "  'duration': 4.96},\n",
              " {'text': 'that I was saying earlier this is the',\n",
              "  'start': 1928.559,\n",
              "  'duration': 5.12},\n",
              " {'text': 'decision boundary the for the equation',\n",
              "  'start': 1930.96,\n",
              "  'duration': 4.8},\n",
              " {'text': 'of this is it transpose x i equal to',\n",
              "  'start': 1933.679,\n",
              "  'duration': 4.521},\n",
              " {'text': 'zero perfect now we have got everything',\n",
              "  'start': 1935.76,\n",
              "  'duration': 5.279},\n",
              " {'text': 'now given XI we can easily find out um',\n",
              "  'start': 1938.2,\n",
              "  'duration': 4.359},\n",
              " {'text': 'where the line lies and based on the',\n",
              "  'start': 1941.039,\n",
              "  'duration': 3.561},\n",
              " {'text': 'line we have the probability and we know',\n",
              "  'start': 1942.559,\n",
              "  'duration': 3.521},\n",
              " {'text': 'on which side of the line it lies if',\n",
              "  'start': 1944.6,\n",
              "  'duration': 3.76},\n",
              " {'text': \"it's a positive one then why equal to\",\n",
              "  'start': 1946.08,\n",
              "  'duration': 4.24},\n",
              " {'text': 'one the classified point is one and if',\n",
              "  'start': 1948.36,\n",
              "  'duration': 3.279},\n",
              " {'text': \"it's on the negative side it's going to\",\n",
              "  'start': 1950.32,\n",
              "  'duration': 5.719},\n",
              " {'text': 'be zero great amazing now we are done um',\n",
              "  'start': 1951.639,\n",
              "  'duration': 5.841},\n",
              " {'text': 'now you can tackle any problem related',\n",
              "  'start': 1956.039,\n",
              "  'duration': 5.201},\n",
              " {'text': 'to classification and yeah I hope this',\n",
              "  'start': 1957.48,\n",
              "  'duration': 5.84},\n",
              " {'text': 'answered all your doubts about this',\n",
              "  'start': 1961.24,\n",
              "  'duration': 4.439},\n",
              " {'text': 'topic um and if you have any questions',\n",
              "  'start': 1963.32,\n",
              "  'duration': 4.44},\n",
              " {'text': \"please drop them in the comments I'll\",\n",
              "  'start': 1965.679,\n",
              "  'duration': 4.401},\n",
              " {'text': 'try to answer them and yeah please stay',\n",
              "  'start': 1967.76,\n",
              "  'duration': 6.919},\n",
              " {'text': 'tuned for new videos thank you',\n",
              "  'start': 1970.08,\n",
              "  'duration': 4.599}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_joined =  \" \".join([line['text'] for line in transcript])"
      ],
      "metadata": {
        "id": "RFnf97hmgF8f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_joined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "3j8IHAPagF5f",
        "outputId": "ec739241-a3a3-4c54-c32b-130672db4dd9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hello everyone I'm Santi and I'm currently working as an ml engineer so I'll be discussing the derivation and math behind classification which is another top ml algorithm I have covered log likelihood before in this video and I would say it's kind of a prerequisite for this video if you already know about that then without any further Ado let's start so the first thing that comes is what is the problem that we are trying to tackle here so basically we have so basically the first question always is what the data set what does the data set look like so it looks something like this as I said we'll start with the simplest form first so X1 X2 and Y so Y is basically uh either zero or one we'll be tackling binary classification first so why is either zero or one let's say looks something like this and X1 could be 1 2 3 4 maybe and X2 could be anything um. 5.9 1.0 1.1 anything so these X1 X2 are the two features and Y is our Target variable which is either zero or one and we want to predict y given X1 and X2 so now comes the graph exactly what are we trying to find here so we'll try to plot X1 X2 here X1 and X2 and then we'll try to get the uh points here so let's just say these These are the four points that we have so now what we're trying to find is this line because this line would be distinguishing these points suppose this point has y equal to um one and this point has y equal 1 this point has y equal 0 this point has y equal 0 so this is the line that we're trying to find that uh differentiates the one yal 1 from yal 0 because suppose now we have any kind of Point suppose X1 is here something and we have given X1 and X2 and we find it here and we know that this point lies on this side of the line so y will be one it's as simple as that I hope you get this so basically X1 and X2 given X1 and X2 what is our y that is our Target to find given X1 and X2 we want to find y fun fact um so logistic degression is also called classification do you know why um drop your thoughts in the comments okay coming back to this one so this line in um classification is called decision bound which is very important remember this term so what is the definition of this line this line is the maximum distance from all of the points so what do I mean by that so here actually we are tracking the distance between the points so so these these differences these are this is the DI for each point X1 I and X2 I we have this point which is Di and this di we want to maximize because we want the line such that it is it it differentiates the two uh one and zero completely okay so what is the exactly the equation that we're trying to solve here so y i given x i param this is the equation that we trying to find why are we trying to find that because if you remember correctly this is log of this um summation I = 1 to m m being the total number of data points M being the total number of data points so this equation is the log likelihood so this is is very important so that is why we trying to find this and we trying to Define this term so this is actually y i given XI parameterized by Theta this is actually the Bol distribution because either the probability we are trying to kind of find the probability of it being one or zero and b means only two values we can predict one or zero so that is why this is like similar to the Bol distribution now that we have got this out of the way we know what we're trying to find so now comes the DI let's focus on di let's get an equation for finding out Di and formalizing that so if you remember I just now said these are all di so this line this side is a positive side of the line and this side is the negative side of the line so we can either decide um so we can either decide we can decide the direction so let's just say this side of the line is positive and this side of the line is negative and then this di let's say let's denote Z as di so now this is very important to understand so listen carefully so this di it can range from positive Infinity to minus infinity so we trying to find the function so can range from positive Infinity to negative Infinity okay so now if di is zero let's say for a point the point is lying on the line so we don't know whether it's positive or negative whether it's whether it's one or zero let's just say positive one gets classified as one negative is classified as zero so if the point is on the line we don't know whether it's positive or negative we don't know whether it's one or zero so the probability is half so di equal 0 should have the probability as half we are now trying to find the constraints for for finding the function that will fit di perfectly so di0 p is equal to half what if di is positive Infinity which means that probability will be one and then di suppose it's negative Infinity then probability will be zero of getting classified as one we are finding that we are now currently thinking of p as the probability of getting classified as one this one so if it's on the line then it see and probability is half if it's Infinity which means we know for sure that it's one point is one so p is equal to 1 and if it's negative Infinity we know for sure that the point can never be one so it's B is zero so now what function fits all these conditions yes so that is the sigmoid function write this down this is the sigmoid function so what does this look like so now we going to see how the sigmo function looks like we denote It generally as G as a function of Z it would be 1 upon 1 + e^ minus Z this is a sigmoid function so let's put in the values and see if it satisfies it or not that we just now saw so if Z equals z then what is gz it is half perfect if Z is infinity what is gz gz is one perfect because because 1 by 1 + e^ minus infinity this would tend to zero so we have 1 and if Z is minus infinity then g z is equal to Z why because 1 by 1 + E power Infinity this is this tends to Infinity so 1 by Infinity is zero so now we have a function which satisfies all these points so now we have our idle function so Sigmar function is what we are going to use here before I delete this let's see the curve of the sigmoid function so this is one this is Zer this is half so at half we have um at half we at zero Z equal to Z so this is z this is gz so when Z equal to Z then we have half when Z equal to Infinity we have one and when Z equal to minus infinity we have zero so somewhat like this is the sigmoid curve remember this is very important comes in interviews comes in placements wherever jobs internships this is extremely important College exams gate exams whatever so this is very important um so now we have the sigmoid function out of our way great one down so now we'll find out the log likelihood so probability of Yi = to 1 given x i parameter Iz by Theta probability of y equal 1 given x i parameterized by Theta earli it was probability of Yi given XI parameterized by Theta because Yi was continuous in distinct case it's a little bit different so we'll take yal yal 1 and we'll take y equal 0 and we'll get the formula for these two then we'll see what to do how to combine them together that's very interesting as well we'll come to that so this is the yal 1 given XI parameterized by Theta so we know we just now found this 1 + e ^ so now what is this term what is this term so basically if you remember when I was talking about the decision boundary in binary classification is going to be a straight line obviously because we trying to divide these points this distance we trying to maximize this distance we're trying to maximize from all the points so this is a straight line so now we know for a function which is the equation of a straight line is y equal to mx + C but this is only when uh it's a it has only one feature which is X so we have here two features so then what what are we going to do so this is going to be y = to Theta 1 X1 + Theta 2 X2 + Theta 0 so we have X1 and X2 as I showed earlier in our data set and so we we will use this equation so basically what does this equation mean and how can we write this equation so I'll just play a short video here where I've talked about how we can um get this equation and write it in the form of yal theta transpose X so here it starts it's just a few seconds video so yeah so you'll understand why we are writing y equal Theta transpose X from this equation now there's a interesting way to write this so we want to write it in the form of a vector so that we have um simple equations to solve instead of this complicated equation with all the variables so now okay so now Theta how do we represent Theta so Theta would be n + one right total number of thetas will be n+ 1 because Theta 0 to Theta n for each of the X1 to xn feature so Theta Matrix would be Theta 0 Theta 1 till Theta n 1 uh n + 1 CR 1 this is our Theta Matrix and our X Matrix looks like this I already said M cross n right M being the number of data points and N being the number of features so now our y would be M so now if you can see carefully so um so Theta 0 has one this is very important try to understand this this is the trick that you will use in most of the algorithms when you're trying to find the uh any and solve using the vector formula so this is the one that we have so let's say if we had x z it would have been one so what we'll do is We'll add a column here of ones so then we'll we'll make this let's make this so this would be these are the nend features we have now we'll have another column of Wes so this becomes + one so this becomes n plus one so now you can see this is M cross1 this is M CR n + 1 n+ 1 CR 1 so we have a matrix multiplication here so if we have Theta transpose what would Theta transpose be like 1 cross n + 1 and then we'll have X so we can write X in the form of like n+ one cross M as well so that just depends on how you express X let's do that suppose we want to write X in the form of um n + 1 cross l n + 1 cross M so this is our final x uh coordinates so this would be n + 1 cross M now we have 1 cross M which is our y so we can say y transpose or we can represent it in the same way as X so in that case y would be 1 cross n so it doesn't really matter when you're trying to solve this is just the way we choose to express this formula and we choose to relate uh y and x and Theta so this would be 1 cross M so now you have this 1 cross M so you have this equation which which is like very common y equal theeta transpose X so now that we know this we know that here we are going to have since this is the probability Yi given XI parameterized by Theta so this is going to be Theta transpose XI because Yi is one that we focusing right now that is why this is the positive distance yeah so this is the positive distance that we are using Y 1 + e^ minus Theta transpose XI why are we doing that so basically what we have done means if you remember di we have written that as Theta transpose x i we don't want to get in this perpendicular thing so we are just going to take theeta transpose XI and then which is kind of like removing all the constants Yi so y equal 1 given XI parameterized by Theta is 1 by 1 e^ minus Theta transpose XI which is our distance positive distance great now the next thing is you can already guess what this is going to look like 1 by one 1 + e the power minus of minus Theta transpose X basically this is G of minus Theta transpose XI and this is this was G of theta transpose x side remember G which is the sigmoid function minus so this is going to be 1 by 1 + e to^ Theta transpose X great so now we have this out of the way it's very important to get the intuition for this because once you understand this U 90% of the problem is done so yeah so we are 90% done so now comes the gradient descent uh function and how we are going to use this log likelihood function that we just now found out um in order to find out the parameters that is Theta 1 and Theta 2 that is Theta basically how we are going to use that so we'll be using radiant descent obviously that is the only way here so now we need to formalize that so um basically maximizing log likelihood is Sim is same as minimizing the loss function so I hope you've seen the video where I've explained why this is the case and I'll reiterate it if you do not know this then it's important to know this before you proceed okay so I I'm going to assume that you know this so basically maximizing the log likelihood is the same as minimizing the loss function so now we are going to write a function for the log likelihood so what we had earlier was likelihood and of two distinct cases now we want to combine them together so this combination trying to understand this is very very very important important I cannot stress it enough okay so I'm going to use the formula directly here is not much the previous uh page where I was discussing it was the likelihood function and now this is the log likelihood just using a log this the function summation I = 1 to M log of probability of y i given x i parameterized by Theta great so now we are going to break this down such that we can accommod two cases listen very carefully so this is the first one so when y = to 1 we want to take the case when y equal to 1 here so basically our data set is divided into points such that y equal 1 and some have y equal 0 with some some points X1 and X2 so we want to find out those points which have yal 1 and take them here in the first part of the function so how are we going to indicate that we are going to use we are going to use something called an indicator variable so I is basically so let me just go into this a little um in detail so basically what is an indicator variable so suppose you have a list of functions suppose you have y here so 1 1 1 0 0 let's say this is our Y data set and we're using the indicator variable so if we do here one of y i equal 1 this means that um this means that whether Y is equal to 1 or not so this is true so this will take the value one here also it will take the value one here also it will take the value of one now here is y1 y i equal to one no so this will take the value zero and this will take the value zero so do it with me so if y i = to 1 I equal 1 2 there are five points point so then the answer would be what 1 + 1 + 1+ 0 + 0 which is going to be three so this is the this is the basic Funda of um indicator variable so now that we know indicator variable so now that we know indicator variable we can move on to the next part of this function so what is going to look like now you can guess it I think you can guess it Yi equal to0 given x i parameterized by Theta this would be indicator variable of Yi equal to Z great so whenever Yi is one we are going to use this because probability of Yi being one parameterized by Theta given x i and whenever Yi is zero we're going to use this so now this log likelihood can be expressed like this very easily now the work is almost done I can say it's 95% done so now what we want to do is just put in this value we know this value so let me just um let me just clean this up okay so now what we going to do is do the substitution and then differentiate that and equate it to zero right how we know this one probability of y i = 1 a given x i parameterized by Theta 1 by 1 + e^ minus Theta transpose X I just nowar wrote it and the other one is probability of y i = 0 given x i parameterized by Theta is 1 by 1 + e^ theeta transal x side okay so now what we going to do is substitute so this this function would look something like this this would be Yi by Yi itself because you saw that when I was showing that graph so if Yi is one then this will be one right and if it Y is zero this will be zero so we can just take Yi and then put in the function here 1 by 1 and for this one we know what is going to be you can guess it 1 minus Yi so if Y is 1 this will be zero which is what we want if Y is zero this would be one which is exactly what we want perfect so now we have 1 - Yi log of 1 by 1 + e^ th transpose x i now we have this great so now as soon as we have this our work is done so it's now 100% done so we just need to differentiate this and equate it to zero and then we'll have an equation for our thetas and we can just use that um we can use it like in gradient descent as you know so I I'll just come to that so first let's simplify this before we differentiate this minus is because I'm putting it here and this would be um this would have a minus already so we can just y i- 1 cool um yeah so now let's remember this for fora now that we have this formula then let's differentiate that Del L equal to Z I'm using Delta again because um we have multiple Dimensions um okay so now we'll differentiate this one and equate it to zero so let's differentiate we have this equation this is basically our LL Theta and this moment and we now want to differentiate this so let's do that so summation I = 1 to M - y i we differentiating with respect to Theta so this would be this would get differentiated so as we know differentiation of log X differentiation what is differentiation of log X it's one one upon X right so DDX of log X is 1x X so that is what we trying to use here and if and one more thing we're trying to use is DDX of so f of x then it is going to be DF DX upon X this is going to go down and then this will be DF upon DX so let's use that so we put this down and now we're going to differentiate this with respect to um with respect to Theta yeah so with respect to Theta what would that look like this would be zero and this would be e^ minus Theta transpose x i and this would be- XI correct yeah got it so this one this one done now the next one next one so what would this look like let's see so Yus one would be in that way keep that oh the same thing so the below it will go e to the Theta transpose XI and here it will be Theta transpose XI multipli with X I so this is the way it goes and then we're differentiating this one so e to the^ X differentiation is e to the^ X X if you know DDX of e^ X is e to^ x that we're trying to use here so e to the^ X differentiation would be this and then differentiating this with respect to Theta would be XI great so we now I've got both of these out of the way this would get plus because of two minuses now um yeah so now if you want see so this term and this term is similar so we're going to glove them together now we are going to use a trick listen very carefully otherwise you'll miss it listen very carefully I'm going to use a very simple trick very old trick in the dictionary but I'm going to use that so if you see here we have what do we have e to the^ minus x 1 plus e to the power - x right so now if I let's do that so if I convert this 1 by e to the power x upon 1 + 1 by e to^ x now get this up here we would have 1 by e^ Min - x we're multiplying with e the^ Min - x we are multiplying e the^ x so we would have e to^ x + 1 correct these two are same you agree so now we are going to use the same thing with um e^ X so e to^ x we have 1 + e to the^ X so now we are going to do is 1 by e ^ - x upon 1 by e ^ - x + 1 so now what will this be let's multiply e^ Min - x we would have 1 by 1 + e^ - x so does this does this look familiar so now if we convert this one like this we have the same denominator and we can combine these terms excellent right remember this trick this will come in handy later as well great so now you just saw what I did so I'm going to use that I'm going to convert this one into this form you will soon realize why am doing this one and not the opposite y i eus Theta transpose x i 1 + eus Theta transpose x i mtip with x i and this one I = 1 to m y i - one let's do this one so what is this going to look like 1 by 1 + e ^ minus Theta transpose x correct you got it this this is extremely a very neat trick that we use and it solves most of the problems as you just saw so now we have the same denominator we can combine these two terms so um yeah so let's do that so let's simplify so what do we have here let's see we have 1 by 1 + e^ th transpose XI and here we have Yi XI from here e^ minus Theta transpose x i we have plus y i x i from here and we have minus XI so as you can see here if you just take yxi common we have 1 plus e to the^ minus Theta transpose x i yes we are very very close to the formula that we want the simplified form so let's do that y i x i let's take that common and we have 1 + e^ minus Theta transpose x i upon 1 + e^ minus Theta transpose x i these two get canceled and this is - x i upon 1 + e^ minus Theta transpose x i we are nearing the term that we trying to find so do not forget this though it's very important this derivation is extremely important is gold pure gold so great now that we have this so we are trying to find out yeah yeah yeah we are almost we are we like literally at the end so I'll just take this one out put it here so this is going to be let's take XI out so we are going to have y i - 1 by 1 + e^ minus Theta transpose x i multip with x i does this look same does this look familiar this is what this is the sigma function correct so let's put that in here y i minus G of theta transpose x i multiplied x i summ i = 1 to n perfect this is the formula that we are that we were looking for now that we have this we know what Delta Theta LL Theta looks like and we will just now um use our gradient dist formula so remember this formula we have spent a lot of time and a lot of effort and hard work to get here so this is very very very important and if you've got this then you practically understood classification you have got it yeah perfect now uh now let's let's do that so now we are going to do the radiant desent so if you know what is gradient descent so basically gradient descent means uh I'll just repeat it like in short I have told about it I've spoken about it in my earlier videos but I'll just tell about it in short so this is where we are starting so this is our loss or our L log likelihood that we are starting okay so if we not talking about log likelihood then this would not be the curve in that case because in log likelihood we're trying to maximize it in loss function we're trying to minimize it so the curve would look something like this so here we are currently this is our log likelihood and we're trying to reach this point which is our top Target which is our maximum log likelihood function so if you have this so this is the Target that we're trying to achieve so what are we going to do simple so we are going to use our gradient descent formula Theta t + 1 so we'll initialize Theta with some random values or zero and then we'll follow this curve yeah so we are going to follow this curve so Theta t + 1 is going to be Theta T minusa Delta Theta LL Theta um sorry minus of this so practically it's plus why is this not said we are going not in the direction of the slope we going um we are going to maximize the likelihood that is why we need to move in this direction which is very important so now this is our gradient descent formula so I'll just write it out again if this is not very cleara what you just need to do is just place the function that we just now derived in here and you have it and you can follow it for as many as you want until you get convergence this is the learning rate and learning rate determines the rate of convergence and we have Theta at the end of this perfect we've literally come this far then you deserve a methal you are a genius you've understood everything and you can tackle any problem of classification now between in interview in placement in job in Gate exam in college exam anything great great amazing so now just a simple thing so basically now that we have Theta we know that the equation of the line is going to be Theta transpose X if you remember that I was using earlier Theta transpose x correct so now it's very important to find out so now we want to find out the equation of the line right so just remember one thing that we did so when I was saying that our di is Theta transpose XI right so and and G of di or G of C transpose X I would be half if di is zero remember this right so let's let's get it get that from here so 1 by 1 + e^ minus di is the transpose x i so I'm going to put this in here equal to half now solving the equation what we have it is the transpose e the^ minus thepose x i 1+ is equal to 2 so this would be 1 so now Theta transpose x i is equal to Z which is exactly the equation of the line that we're looking for and this will be the decision boundary you remember this one that I was saying earlier this is the decision boundary the for the equation of this is it transpose x i equal to zero perfect now we have got everything now given XI we can easily find out um where the line lies and based on the line we have the probability and we know on which side of the line it lies if it's a positive one then why equal to one the classified point is one and if it's on the negative side it's going to be zero great amazing now we are done um now you can tackle any problem related to classification and yeah I hope this answered all your doubts about this topic um and if you have any questions please drop them in the comments I'll try to answer them and yeah please stay tuned for new videos thank you\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESTORING the PUNCTUATION\n"
      ],
      "metadata": {
        "id": "ERBLtV0ugbhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepmultilingualpunctuation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "KoESYj1IgFs1",
        "outputId": "4e8b5b01-babd-49c0-ec18-73460264d1bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepmultilingualpunctuation\n",
            "  Downloading deepmultilingualpunctuation-1.0.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from deepmultilingualpunctuation) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from deepmultilingualpunctuation) (4.53.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.1->deepmultilingualpunctuation)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->deepmultilingualpunctuation) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.1->deepmultilingualpunctuation) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->deepmultilingualpunctuation) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->deepmultilingualpunctuation) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.1->deepmultilingualpunctuation) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->deepmultilingualpunctuation) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->deepmultilingualpunctuation) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->deepmultilingualpunctuation) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->deepmultilingualpunctuation) (2025.6.15)\n",
            "Downloading deepmultilingualpunctuation-1.0.1-py3-none-any.whl (5.4 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deepmultilingualpunctuation\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed deepmultilingualpunctuation-1.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "7ccb09a070e0461896ecc793069a48b6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "\n",
        "model = PunctuationModel()\n",
        "\n",
        "result = model.restore_punctuation(transcript_joined)\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420,
          "referenced_widgets": [
            "517504d8039749d2a81727e9150605b7",
            "9d8fcd6808644ee1bc7725a1d3a7e333",
            "3598bd341f6a41acb163d2b0bc13ad16",
            "7c7f5e9e7110420b81e6cd1bcc66a59b",
            "7a3481e7c4d94e40854fc0cb1ab262bf",
            "bbf1c1794fff49fda7ed56d822e84fb9",
            "ae913af498594650867ebd5843d5834f",
            "2072a86d141c4ad887fffca7fcd1eec1",
            "a47f6a9e3cf4480992ed0509183335dd",
            "c2bbcddfc82a4ad085397953d726e1aa",
            "fcc74829380546379f89a5aa51036640",
            "476bd5420a0f4e3590af40093648b632",
            "66ef0bc7add141819c519e9c90b88fc2",
            "b70ceb839b41424eb30e8b396682ea72",
            "f116664e770743af8da3399ae8524c93",
            "0305f9d328cd4b65b96dd899e277f945",
            "f4c6fc8a5fb0441bb472a00be949ab24",
            "78ce6cad954b49519d0009c0d77b854f",
            "f9bceb67dabb4b7297d2abb31402b4f8",
            "2758542e21794dd38b29e426c4964f31",
            "04ea1681f35c4968b17273426ca78cfa",
            "b9b5846b1f67422497bada4cd8823940",
            "b5a6026e9aa44736ba6207c43dedb1b7",
            "317075e79cb148aca4adad556bb751d3",
            "8e3a0359555247d7b0b339bd1b00a665",
            "eed49d30f7ee4bdd876324dca8db4b0d",
            "8f71a549394a422283e905ad79ec86e0",
            "57a8b0d03e724cd09333c446df5262fd",
            "0622faf29f984d6eb6beb020b68de533",
            "e16084076aa240ad82cf72d4291ab2bf",
            "99b4a7cdcf3d401c912e7f6b3f78de63",
            "d20d410313744be7a9fca0707b3a90ad",
            "3dace62d8da9451ea370317f8d4464a5",
            "93daf05fb7ee49d1ad9e276fa30856d0",
            "3a0185284afb44b485c721d92f86f207",
            "ee7b3c6de1f64aaa9d68d2771f4b87c7",
            "8c7d7bfd00c64f7bb076b90d7f621174",
            "e0746e9c7d1f4c24abc3969aaae3ff33",
            "65e62518619d4a45bb596613a74ec178",
            "3e906831985d45cda4e856676220fcfd",
            "f90dcd4da5804c3cbca57298b404c132",
            "baea2f6f58bd497f9c154d65e7f5b120",
            "822545be41714e5291d046ce220b2411",
            "7e55e323a8aa425da005b3208f680e0a",
            "f2a4ea28ba824340ab56cb178597e564",
            "5eaf52c440fc49c8ac6f307b91abad6d",
            "fe282e01871948e0bc34fca923fa94e6",
            "90d2a1be326f44fe9dc2004446f8abf6",
            "22b8c35dd9354beea8cb661bfd43634e",
            "66f93f8984a74790b015443d222e3588",
            "5050112d34094c7c91c22f8cad906386",
            "6d547d1eb2dd4eebba75bb62c541eb27",
            "d1c27199d4b240fea13d8cd49f18a702",
            "5e1c10f144204ffeba3a54ec55959beb",
            "3e895b19a36b472799287c26cf790841",
            "25e626a201f4451583535339b25c1c8b",
            "4d4a0cd2a5e94d8f8c1690c49b887a85",
            "6faec6565fb84d429deea644fa3bbdc1",
            "95663d9ee6e1442a819ec23de0926f5a",
            "1e0f25ff4cf640a494f6d0030b13e413",
            "798a09f3d09842c2b3ba308e195e666f",
            "88c9865737d34ed8bd22f37c3aeebdc5",
            "ac0e91732a314eb281ef1adf5869a97c",
            "a7269606d40143f7a2775ae9a30c5bc9",
            "fb133df0264d481b89792a11152fc2ee",
            "9bddc141e3894839b7610321be74ac18"
          ]
        },
        "collapsed": true,
        "id": "Mh5bXVVgpZVR",
        "outputId": "594e0cc8-4cc7-4cdf-f25d-181ac509ea31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "517504d8039749d2a81727e9150605b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "476bd5420a0f4e3590af40093648b632"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/406 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5a6026e9aa44736ba6207c43dedb1b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93daf05fb7ee49d1ad9e276fa30856d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2a4ea28ba824340ab56cb178597e564"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25e626a201f4451583535339b25c1c8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:181: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
            "  warnings.warn(\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello everyone. I'm Santi and I'm currently working as an ml engineer, so I'll be discussing the derivation and math behind classification, which is another top ml algorithm. I have covered log likelihood before in this video and I would say it's kind of a prerequisite for this video. if you already know about that, then without any further Ado, let's start. so the first thing that comes is: what is the problem that we are trying to tackle here? so basically we have. so basically, the first question always is: what the data set? what does the data set look like? so it looks something like this. as I said, we'll start with the simplest form first. so X1, X2 and Y. so Y is basically, uh, either zero or one. we'll be tackling binary classification first. so why is either zero or one? let's say, looks something like this: and X1 could be 1, 2, 3, 4 maybe, and X2 could be anything um 5.9, 1.0, 1.1, anything. so these X1, X2 are the two features and Y is our Target variable, which is either zero or one, and we want to predict y, given X1 and X2. so now comes the graph: exactly what are we trying to find here? so we'll try to plot X1, X2 here, X1 and X2, and then we'll try to get the uh points here. so let's just say these: These are the four points that we have. so now what we're trying to find is this line, because this line would be distinguishing these points: suppose this point has y equal to um one and this point has y equal 1. this point has y equal 0. this point has y equal 0. so this is the line that we're trying to find that uh differentiates the one, yal 1, from yal 0, because suppose now we have any kind of Point, suppose X1 is here something, and we have given X1 and X2, and we find it here, and we know that this point lies on this side of the line. so y will be one. it's as simple as that. I hope you get this. so basically, X1 and X2, given X1 and X2, what is our y? that is our Target to find. given X1 and X2, we want to find y. fun fact, um. so logistic degression is also called classification. do you know why? um, drop your thoughts in the comments. okay, coming back to this one. so this line in um classification is called decision bound, which is very important. remember this term. so what is the definition of this line? this line is the maximum distance from all of the points. so what do I mean by that? so here, actually, we are tracking the distance between the points. so so, these, these differences, these are. this is the DI for each point: X1 I and X2 I. we have this point which is Di, and this di we want to maximize because we want the line such that it is it, it differentiates the two, uh, one and zero, completely okay. so what is the exactly the equation that we're trying to solve here? so y, i given x, i param. this is the equation that we trying to find. why are we trying to find that? because, if you remember correctly, this is log of this um summation: I = 1 to m, m being the total number of data points, M being the total number of data points. so this equation is the log likelihood. so this is is very important. so that is why we trying to find this and we trying to Define this term. so this is actually y i given XI, parameterized by Theta. this is actually the Bol distribution, because either the probability- we are trying to kind of find the probability of it being one or zero- and b means only two values we can predict: one or zero. so that is why this is like similar to the Bol distribution. now that we have got this out of the way, we know what we're trying to find. so now comes the DI. let's focus on di, let's get an equation for finding out Di and formalizing that. so, if you remember, I just now said these are all di. so this line, this side, is a positive side of the line and this side is the negative side of the line. so we can either decide um, so we can either decide, we can decide the direction. so let's just say this side of the line is positive and this side of the line is negative. and then this di, let's say let's denote Z as di. so now, this is very important to understand. so listen carefully. so this di, it can range from positive Infinity to minus infinity. so we trying to find the function. so can range from positive Infinity to negative Infinity. okay, so now if di is zero, let's say for a point, the point is lying on the line, so we don't know whether it's positive or negative, whether it's whether it's one or zero. let's just say positive one gets classified as one, negative is classified as zero. so if the point is on the line, we don't know whether it's positive or negative, we don't know whether it's one or zero, so the probability is half. so di equal 0 should have the probability as half. we are now trying to find the constraints for, for finding the function that will fit di perfectly. so di0, p is equal to half. what if di is positive, Infinity, which means that probability will be one, and then di, suppose it's negative Infinity, then probability will be zero of getting classified as one. we are finding that we are now currently thinking of p as the probability of getting classified as one, this one. so if it's on the line, then it see, and probability is half. if it's Infinity, which means we know for sure that it's one point is one, so p is equal to 1. and if it's negative Infinity, we know for sure that the point can never be one, so it's B is zero. so now, what function fits all these conditions? yes, so that is the sigmoid function. write this down. this is the sigmoid function. so what does this look like? so now we going to see how the sigmo function looks like. we denote It generally as G, as a function of Z. it would be 1 upon 1 + e^ minus Z. this is a sigmoid function, so let's put in the values and see if it satisfies it or not that we just now saw. so if Z equals z, then what is gz? it is half perfect. if Z is infinity, what is gz? gz is one perfect because because 1 by 1 + e^ minus infinity, this would tend to zero. so we have 1. and if Z is minus infinity, then g? z is equal to Z. why? because 1 by 1 + E power Infinity. this is, this tends to Infinity. so 1 by Infinity is zero. so now we have a function which satisfies all these points. so now we have our idle function. so Sigmar function is what we are going to use here. before I delete this, let's see the curve of the sigmoid function. so this is one, this is Zer, this is half. so at half we have um, at half we at zero, Z equal to Z. so this is z, this is gz. so when Z equal to Z, then we have half. when Z equal to Infinity, we have one, and when Z equal to minus infinity, we have zero. so somewhat like: this is the sigmoid curve. remember, this is very important. comes in interviews, comes in placements wherever, jobs, internships. this is extremely important College exams, gate exams, whatever. so this is very important. um, so now we have the sigmoid function out of our way, great one down. so now we'll find out the log likelihood. so probability of Yi = to 1 given x i parameter, Iz by Theta. probability of y equal 1 given x i parameterized by Theta. earli, it was probability of Yi given XI parameterized by Theta, because Yi was continuous. in distinct case it's a little bit different. so we'll take yal, yal 1, and we'll take y equal 0 and we'll get the formula for these two. then we'll see what to do, how to combine them together. that's very interesting as well. we'll come to that. so this is the yal 1 given XI parameterized by Theta. so we know. we just now found this: 1 + e ^. so now, what is this term? what is this term? so, basically, if you remember when I was talking about the decision boundary in binary classification is going to be a straight line, obviously, because we trying to divide these points, this distance, we trying to maximize this distance, we're trying to maximize from all the points. so this is a straight line. so now we know, for a function which is the equation of a straight line, is y equal to mx + C, but this is only when, uh, it's a. it has only one feature, which is X. so we have here two features. so then what? what are we going to do? so this is going to be y = to Theta 1, X1 + Theta 2, X2 + Theta 0, so we have X1 and X2, as I showed earlier in our data set, and so we we will use this equation. so, basically, what does this equation mean and how can we write this equation? so I'll just play a short video here where I've talked about how we can um get this equation and write it in the form of yal, theta transpose X. so here it starts. it's just a few seconds video. so yeah, so you'll understand why we are writing y equal Theta transpose X from this equation. now there's a interesting way to write this, so we want to write it in the form of a vector, so that we have um simple equations to solve instead of this complicated equation with all the variables. so now, okay, so now Theta. how do we represent Theta? so Theta would be n + one right total number of thetas will be n+ 1, because Theta 0 to Theta n for each of the X1 to xn feature. so Theta Matrix would be Theta 0, Theta 1 till Theta n, 1, uh n + 1, CR 1. this is our Theta Matrix and our X Matrix looks like this. I already said M cross, n, right, M being the number of data points and N being the number of features. so now our y would be M. so now, if you can see carefully, so um, so Theta 0 has one. this is very important. try to understand this. this is the trick that you will use in most of the algorithms when you're trying to find the uh any and solve using the vector formula. so this is the one that we have. so let's say, if we had x, z, it would have been one. so what we'll do is We'll add a column here of ones. so then we'll, we'll make this, let's make this. so this would be: these are the nend features we have. now we'll have another column of Wes, so this becomes + one. so this becomes n plus one. so now you can see this is M cross1. this is M CR n + 1, n+ 1, CR 1. so we have a matrix multiplication here. so if we have Theta transpose, what would Theta transpose be like? 1 cross n + 1, and then we'll have X. so we can write X in the form of like n+ one cross M as well. so that just depends on how you express X. let's do that. suppose we want to write X in the form of um n + 1 cross l, n + 1 cross M. so this is our final x uh coordinates. so this would be n + 1 cross M. now we have 1 cross M, which is our y. so we can say y transpose, or we can represent it in the same way as X. so in that case y would be 1 cross n. so it doesn't really matter when you're trying to solve, this is just the way we choose to express this formula and we choose to relate uh, y and x and Theta. so this would be 1 cross M. so now you have this 1 cross M. so you have this equation which, which is like very common, y equal, theeta transpose X. so now that we know this, we know that here we are going to have- since this is the probability- Yi given XI, parameterized by Theta. so this is going to be Theta transpose XI, because Yi is one that we focusing right now. that is why this is the positive distance. yeah, so this is the positive distance that we are using: Y, 1 + e^ minus Theta transpose XI. why are we doing that? so, basically, what we have done means, if you remember di, we have written that as Theta transpose x i. we don't want to get in this perpendicular thing, so we are just going to take theeta transpose XI and then, which is kind of like removing all the constants Yi, so y equal 1, given XI, parameterized by Theta, is 1 by 1 e^ minus Theta transpose XI, which is our distance, positive distance. great. now the next thing is you can already guess what this is going to look like: 1 by one, 1 + e, the power minus of minus Theta transpose X. basically, this is G of minus Theta transpose XI. and this is this was G of theta transpose x side, remember G, which is the sigmoid function, minus. so this is going to be 1 by 1 + e to^ Theta transpose X. great. so now we have this out of the way. it's very important to get the intuition for this, because once you understand this U, 90% of the problem is done. so, yeah, so we are 90% done. so now comes the gradient descent uh function and how we are going to use this log likelihood function that we just now found out, um, in order to find out the parameters. that is Theta 1 and Theta 2. that is Theta, basically how we are going to use that. so we'll be using radiant descent. obviously that is the only way here. so now we need to formalize that. so, um, basically, maximizing log likelihood is Sim is same as minimizing the loss function. so I hope you've seen the video where I've explained why this is the case and I'll reiterate it. if you do not know this, then it's important to know this before you proceed. okay, so I I'm going to assume that you know this. so, basically, maximizing the log likelihood is the same as minimizing the loss function. so now we are going to write a function for the log likelihood. so what we had earlier was likelihood and of two distinct cases. now we want to combine them together. so this combination- trying to understand this- is very, very, very important, important. I cannot stress it enough, okay, so I'm going to use the formula directly. here is not much. the previous uh page where I was discussing it was the likelihood function and now this is the log likelihood, just using a log. this the function summation: I = 1 to M. log of probability of y, i given x, i parameterized by Theta great. so now we are going to break this down such that we can accommod two cases. listen very carefully. so this is the first one. so when y = to 1, we want to take the case when y equal to 1 here. so basically, our data set is divided into points such that y equal 1 and some have y equal 0, with some some points X1 and X2. so we want to find out those points which have yal 1 and take them here in the first part of the function. so how are we going to indicate that we are going to use? we are going to use something called an indicator variable. so I is basically. so let me just go into this a little um in detail. so basically, what is an indicator variable? so suppose you have a list of functions, suppose you have y here, so 1, 1, 1, 0, 0. let's say this is our Y data set and we're using the indicator variable. so if we do here one of y, i equal 1, this means that, um, this means that whether Y is equal to 1 or not, so this is true. so this will take the value one. here also, it will take the value one. here also, it will take the value of one. now here is y1. y i equal to one. no, so this will take the value zero and this will take the value zero. so do it with me. so if y i = to 1, I equal 1, 2. there are five points, point. so then the answer would be what? 1 + 1 + 1+ 0 + 0, which is going to be three. so this is the. this is the basic Funda of um indicator variable. so now that we know indicator variable, so now that we know indicator variable, we can move on to the next part of this function. so what is going to look like now? you can guess it. I think you can guess it. Yi equal to0, given x i parameterized by Theta, this would be indicator variable of Yi equal to Z. great. so whenever Yi is one, we are going to use this, because probability of Yi being one parameterized by Theta, given x i, and whenever Yi is zero, we're going to use this. so now this log likelihood can be expressed like this very easily. now the work is almost done. I can say it's 95% done. so now what we want to do is just put in this value. we know this value, so let me just, um, let me just clean this up, okay. so now what we going to do is do the substitution and then differentiate that and equate it to zero. right, how we know this? one: probability of y i = 1 a given x i, parameterized by Theta, 1 by 1 + e^ minus Theta, transpose X. I just nowar wrote it. and the other one is probability of y i = 0, given x i, parameterized by Theta, is 1 by 1 + e^ theeta transal x side. okay, so now what we going to do is substitute. so this, this function, would look something like this: this would be Yi by Yi itself, because you saw that when I was showing that graph. so if Yi is one, then this will be one, right, and if it Y is zero, this will be zero. so we can just take Yi and then put in the function here 1 by 1, and for this one we know what is going to be, you can guess it: 1 minus Yi. so if Y is 1, this will be zero, which is what we want. if Y is zero, this would be one, which is exactly what we want, perfect. so now we have 1 - Yi log of 1 by 1 + e^ th, transpose x, i. now we have this. great. so now, as soon as we have this, our work is done. so it's now 100% done. so we just need to differentiate this and equate it to zero and then we'll have an equation for our thetas and we can just use that um, we can use it like in gradient descent, as you know. so I I'll just come to that. so first, let's simplify this before we differentiate. this minus is because I'm putting it here and this would be um, this would have a minus already. so we can just y i- 1, cool, um, yeah. so now let's remember this for fora, now that we have this formula, then let's differentiate that Del L equal to Z. I'm using Delta again because, um, we have multiple Dimensions, um, okay. so now we'll differentiate this one and equate it to zero. so let's differentiate. we have this equation, this is basically our LL Theta and this moment, and we now want to differentiate this. so let's do that. so, summation: I = 1 to M - y i. we differentiating with respect to Theta. so this would be, this would get differentiated. so, as we know, differentiation of log X, differentiation. what is differentiation of log X, it's one one upon X, right? so DDX of log X is 1x X. so that is what we trying to use here. and if- and one more thing we're trying to use is DDX of, so f of x, then it is going to be DF, DX upon X. this is going to go down and then this will be DF upon DX. so let's use that. so we put this down. and now we're going to differentiate this with respect to um, with respect to Theta. yeah, so with respect to Theta, what would that look like? this would be zero and this would be: e^ minus Theta transpose x, i, and this would be- XI, correct? yeah, got it. so this one, this one, done. now the next one, next one. so what would this look like? let's see. so, Yus, one would be in that way. keep that. oh, the same thing. so the below it will go: e to the Theta transpose XI, and here it will be: Theta transpose XI, multipli with X I. so this is the way it goes. and then we're differentiating this one. so e to the^ X differentiation is e to the^ X, X. if you know, DDX of e^ X is e to^ x that we're trying to use here. so e to the^ X differentiation would be this: and then differentiating this with respect to Theta would be XI. great, so we- now I've got both of these out of the way- this would get plus because of two minuses. now, um, yeah, so now, if you want see. so this term and this term is similar, so we're going to glove them together. now we are going to use a trick. listen very carefully, otherwise you'll miss it. listen very carefully. I'm going to use a very simple trick, very old trick in the dictionary, but I'm going to use that. so if you see, here we have what do? we have? e to the^ minus x, 1 plus e to the power - x, right, so now, if I let's do that, so if I convert this 1 by e to the power x upon 1 + 1 by e to^ x, now get this up here- we would have 1 by e^ Min - x. we're multiplying with e the^ Min - x, we are multiplying e the^ x. so we would have e to^ x + 1, correct, these two are same, you agree? so now we are going to use the same thing with um e^ X. so e to^ x. we have 1 + e to the^ X. so now we are going to do is 1 by e ^ - x upon 1 by e ^ - x + 1. so now, what will this be? let's multiply e^ Min - x. we would have 1 by 1 + e^ - x. so does this? does this look familiar? so now, if we convert this one like this, we have the same denominator and we can combine these terms. excellent, right, remember this trick. this will come in handy later as well. great, so now you just saw what I did, so I'm going to use that. I'm going to convert this one into this form. you will soon realize why am doing this one and not the opposite: y i, eus. Theta transpose x i 1 + eus. Theta transpose x i. mtip with x i and this one I = 1 to m y i - one. let's do this one. so what is this going to look like? 1 by 1 + e ^ minus Theta transpose x. correct, you got it this. this is extremely a very neat trick that we use and it solves most of the problems, as you just saw. so now we have the same denominator, we can combine these two terms. so, um, yeah, so let's do that. so let's simplify. so what do we have here. let's see: we have 1 by 1 + e^ th transpose XI, and here we have Yi XI from here. e^ minus Theta transpose x i. we have plus y i x i from here and we have minus XI. so, as you can see here, if you just take yxi common, we have 1 plus e to the^ minus Theta, transpose x i. yes, we are very, very close to the formula that we want, the simplified form. so let's do that: y i x i, let's take that common, and we have 1 + e^ minus Theta, transpose x i upon 1 + e^ minus Theta, transpose x i. these two get canceled and this is: - x i upon 1 + e^ minus Theta, transpose x i. we are nearing the term that we trying to find. so do not forget this, though it's very important, this derivation is extremely important. is gold, pure gold? so great now that we have this? so we are trying to find out: yeah, yeah, yeah, we are almost, we are we like literally at the end. so I'll just take this one out, put it here. so this is going to be: let's take XI out, so we are going to have y i - 1 by 1 + e^ minus Theta transpose x. i multip with x. i does this look same? does this look familiar? this is what this is, the sigma function, correct? so let's put that in here. y, i minus G of theta transpose x. i multiplied x. i summ i = 1 to n. perfect, this is the formula that we are, that we were looking for. now that we have this, we know what Delta Theta, LL Theta looks like, and we will just now um use our gradient dist formula. so remember this formula. we have spent a lot of time and a lot of effort and hard work to get here. so this is very, very, very important and if you've got this, then you practically understood classification. you have got it, yeah, perfect now. uh, now let's, let's do that. so now we are going to do the radiant desent. so if you know what is gradient descent? so basically, gradient descent means, uh, I'll just repeat it like in short. I have told about it, I've spoken about it in my earlier videos, but I'll just tell about it in short. so this is where we are starting. so this is our loss or our L log likelihood that we are starting. okay, so if we not talking about log likelihood, then this would not be the curve in that case, because in log likelihood we're trying to maximize it. in loss function we're trying to minimize it. so the curve would look something like this: so here we are currently, this is our log likelihood, and we're trying to reach this point, which is our top Target, which is our maximum log likelihood function. so, if you have this, so this is the Target that we're trying to achieve. so what are we going to do? simple, so we are going to use our gradient descent formula, Theta t + 1. so we'll initialize Theta with some random values or zero, and then we'll follow this curve. yeah, so we are going to follow this curve. so Theta t + 1 is going to be Theta T minusa, Delta, Theta LL, Theta- um, sorry, minus of this, so practically it's plus. why is this not said? we are going not in the direction of the slope, we going, um, we are going to maximize the likelihood. that is why we need to move in this direction, which is very important. so now, this is our gradient descent formula. so I'll just write it out again if this is not very cleara. what you just need to do is just place the function that we just now derived in here and you have it and you can follow it for as many as you want until you get convergence. this is the learning rate, and learning rate determines the rate of convergence. and we have Theta at the end of this. perfect, we've literally come this far. then you deserve a methal. you are a genius, you've understood everything and you can tackle any problem of classification now between in interview, in placement, in job, in Gate exam, in college exam, anything great, great, amazing. so now just a simple thing. so basically, now that we have Theta, we know that the equation of the line is going to be Theta transpose X. if you remember that I was using earlier Theta transpose x, correct? so now it's very important to find out. so now we want to find out the equation of the line, right? so just remember one thing: that we did so when I was saying that our di is Theta transpose XI, right? so, and and G of di or G of C transpose X I would be half if di is zero. remember this, right? so let's, let's get it. get that from here. so 1 by 1 + e^ minus di is the transpose x i. so I'm going to put this in here: equal to half. now solving the equation. what we have. it is the transpose e, the^ minus, thepose x- i, 1+ is equal to 2, so this would be 1. so now, Theta transpose x- i is equal to Z, which is exactly the equation of the line that we're looking for, and this will be the decision boundary- you remember this one that I was saying earlier. this is the decision boundary. the for the equation of this is it transpose x- i equal to zero. perfect, now, we have got everything. now, given XI, we can easily find out, um where the line lies and, based on the line, we have the probability and we know on which side of the line it lies. if it's a positive one, then why? equal to one? the classified point is one, and if it's on the negative side, it's going to be zero. great, amazing. now we are done. um, now you can tackle any problem related to classification and, yeah, I hope this answered all your doubts about this topic, um, and if you have any questions, please drop them in the comments. I'll try to answer them and, yeah, please stay tuned for new videos, thank you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import textwrap\n",
        "\n",
        "# Load the model\n",
        "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=0)\n",
        "\n",
        "# Split into chunks (at most 400 tokens per chunk)\n",
        "def chunk_text(text, max_tokens=400):\n",
        "    return textwrap.wrap(text, max_tokens, break_long_words=False)\n",
        "\n",
        "chunks = chunk_text(final_summary, max_tokens=400)\n",
        "\n",
        "# Ask question on each chunk and gather answers\n",
        "question = \"What is the main topic of this video?\"\n",
        "answers = []\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    prompt = f\"Question: {question} Context: {chunk}\"\n",
        "    try:\n",
        "        ans = pipe(prompt, max_length=100, do_sample=False)[0]['generated_text']\n",
        "        answers.append(ans)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Chunk {i} failed: {e}\")\n",
        "\n",
        "# Combine all answers into a final summary\n",
        "print(\"\\n✅ Final Answer:\\n\", \" \".join(answers))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duIBy5jNrSD3",
        "outputId": "0d01d105-03cd-4a0c-c629-92e21cae4e68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Final Answer:\n",
            " derivation and math behind classification, which is another top ml algorithm. look at the data set first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So we'll be tackling binary classification first. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one. So let's start with the simplest form first. So X1, X2 and Y. So Y is basically, uh, either zero or one yal 1, yal 0, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, yal 1, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x, y, x,  Log likelihood Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the XI distribution. Let's take the  zero. and then this di, let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say let's say point is on the line, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don' whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we don't know whether it's one or zero, we probability of getting classified as one. that the point can never be one. if it's positive Infinity, we know for sure that the point can never be one. if it's negative Infinity, we know for sure that the point can never be one. if it's positive Infinity, we know for sure that the point can never be one. if it's negative Infinity, we know for sure that the point can never be one. if it's positive Infinity, we know for sure that the point can never be one. if it's negative Infinity, we know for sure that the point can never be one. if it's positive Infinity, we know for sure that the point can never be one. can never be one. if it's negative Infinity, we know for sure that the point can never be one. if it's positive Infinity, we know for sure that the point can Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look at the sigmo function. Let's look sigmoid function sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve sigmoid curve yal = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = yal + e = the decision boundary in binary classification is going to be a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize this distance, we're trying to maximize from all the points. So this is a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So this is a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So this is a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So this is a straight line, obviously, because we trying to divide these points, this distance, We're trying to maximize from all the points. So this is a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So now we know, for a function which is the equation of the decision boundary in binary classification is going to be a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So now we know, for a function which is the equation of the decision boundary in binary classification is going to be a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So now we know, for a function which is the equation of the decision boundary in binary classification is going to be a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So now we know, for a function which is the equation of the decision boundary in binary classification is going to be a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points. So a straight line, obviously, because we trying to divide these points, this distance, we're trying to maximize from all the points Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta 0, y = to Theta 1, X1 + Theta 2, X2 + Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Mat Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Matrix: Theta Mat Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. Theta 0 is the number of data points and the number of features. features. Theta 0 is the number of data points and the number of features. The We'll add a column here of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one. Now we'll have another column of Wes, so this becomes + one Column of Wes, so this becomes + one. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X in the form of n + 1 cross l, n + 1 cross M. Let's write X y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y =  y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y = y =  Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta trans Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. minus Theta transpose XI. Theta 11 and Theta 12. Theta 12 and Theta 13. Theta 13 and Theta 14. Theta 14 and Theta 15. Theta 15 and Theta 16. Theta 16 and Theta 17. Theta 17 and Theta 18. Theta 18 and Theta 19. Theta 19 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. Theta 21 and Theta 20. The Let me just go into this a little bit in detail. So basically, what is an indicator variable? indicator variable an indicator variable y13 and y14 and y15 and y15 and y16 and y17 and y18 and y19 and y20 and y21 and y22 and y23 and y24 and y25 and y26 and y27 and y29 and y30 and y27 and y29 and y30 and y27 and y29 and y30 and y27 and y29 and y30 and y27 and y29 and y30 and y27 and y29 and y30 and y27 and y29 and y30 and y27 and y29 and y30 and y27 and y29 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and y30 and log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log likelihood. log Let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, um, let me just, Theta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = 1 by 1 + e theeta = e th, transpose x, i. now we have 1 - Yi log of 1 by 1 + e th, transpose x, i. now we have 1 - Yi log of 1 by 1 + e th, transpose x, i. now we have 1 - Yi log of 1 by 1 + e th, transpose x, i. now we have 1 - Yi log of 1 by 1 + e th, transpose x, i. now we have 1 - Yi log of 1 by 1 + e th, transpose x, i. now we have 1 - Yi log of 1 by 1 + e th, transpose x Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify this. Let's simplify log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, log X, DF upon DX is going to go down and then this will be zero and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpose x, i, and this would be: e minus Theta transpos minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose x, i, and this would be: minus Theta transpose  Theta is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is e to X is if you want see. so this would get plus because of two minuses. now, um, yeah, so now, e to the X differentiation would be this: and then differentiating this with respect to Theta would be XI. great, so now, e to the X differentiation would be this: and then differentiating this with respect to Theta would be XI. great, so now, e to the X differentiation would be this: and then differentiating this with respect to Theta would be XI. great, so now, e to the X differentiation would be this: and then differentiating this with respect to Theta would be XI. great, so now, e to the X differentiation would be this: and then differentiating this with respect to Theta would be XI. great, so now, e to the X differentiation would be this: and then differentiating this with respect to Theta would be XI. great, so now, e to the X differentiation would be this: and then differentiating this with respect to Theta would Theta 1 by 1 + e to x. so now, what will this be? let's multiply e Min - x. we would have 1 by 1 + e to x. so now, what will this be? let's multiply e Min - x. we would have 1 by 1 + e to x. so now, what will this be? let's multiply e Min - x. we would have 1 by 1 + e to x. so now, what will this be? let's multiply e Min - x. we would have 1 by 1 + e to x. so now, what will this be? let's multiply e Min - x. we would have 1 by 1 + e to x. so now, what will this be? let's multiply e Min - x. we would have 1 by 1 + e to x. so now, what will this be? let's multiply e Min - x. we would have 1 by 1 + e to x. so now, what will this be? let's multiply e Min mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m y i - one. let's do this one. mtip with x i and this one I = 1 to m Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, minus Theta, we have 1 plus e to the minus Theta, we have 1 plus e to the minus Theta, we have 1 plus So we are going to take XI out, so we are going to have y i - 1 by 1 + e minus Theta, transpose x i upon 1 + e minus Theta, transpose x i upon 1 + e minus Theta, transpose x i upon 1 + e minus Theta, transpose x i upon 1 + e minus Theta, transpose x i upon 1 + e minus Theta, transpose x i upon 1 + e minus Theta, transpose i multiplied x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 to n. y, i minus G of theta transpose x. i summ i = 1 log likelihood curve re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-initialize Theta with some random values or zero, and we'll re-in Theta t + 1 is going to be Theta T minusa, Delta, Theta LL, Theta- um, sorry, minus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, so practically it's plus of this, derived a gradient descent function. mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathematician, you're a mathem mathematician, you're a mathematician, you're a mathematician, you're We want to find out the equation of the line, right? Theta transpose x- i is equal to Z, which is exactly the equation of the line that we're looking for, and this will be the decision boundary. x- i equal to zero Well, if you're not a big fan of this, you're not going to like it, but if you're a big fan of this, you're not going to like it of this, you're not going to like it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvRd0LZArSIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRzP1lUErSLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}